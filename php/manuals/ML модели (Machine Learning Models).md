# ü§ñ –ü–æ–ª–Ω—ã–π –º–∞–Ω—É–∞–ª –ø–æ –º–æ–¥–µ–ª—è–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: –¢–∏–ø—ã, –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–í–≤–µ–¥–µ–Ω–∏–µ –≤ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ](#–≤–≤–µ–¥–µ–Ω–∏–µ-–≤-–º–∞—à–∏–Ω–Ω–æ–µ-–æ–±—É—á–µ–Ω–∏–µ)
2. [–¢–∏–ø—ã –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è](#—Ç–∏–ø—ã-–∑–∞–¥–∞—á-–º–∞—à–∏–Ω–Ω–æ–≥–æ-–æ–±—É—á–µ–Ω–∏—è)
3. [–û—Å–Ω–æ–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è](#–æ—Å–Ω–æ–≤–Ω—ã–µ-–∞–ª–≥–æ—Ä–∏—Ç–º—ã-–º–∞—à–∏–Ω–Ω–æ–≥–æ-–æ–±—É—á–µ–Ω–∏—è)
4. [–†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏](#—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ-–º–æ–¥–µ–ª–∏)
5. [–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏](#–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ-–º–æ–¥–µ–ª–∏)
6. [–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è](#–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è)
7. [–ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã](#–∞–Ω—Å–∞–º–±–ª–µ–≤—ã–µ-–º–µ—Ç–æ–¥—ã)
8. [–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ](#–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ-—Å–µ—Ç–∏-–∏-–≥–ª—É–±–æ–∫–æ–µ-–æ–±—É—á–µ–Ω–∏–µ)
9. [–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π](#–æ—Ü–µ–Ω–∫–∞-–º–æ–¥–µ–ª–µ–π)
10. [–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã](#–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ-–ø—Ä–∏–º–µ—Ä—ã)
11. [–õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏](#–ª—É—á—à–∏–µ-–ø—Ä–∞–∫—Ç–∏–∫–∏)

## –í–≤–µ–¥–µ–Ω–∏–µ –≤ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

**–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (Machine Learning)** ‚Äî —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ–∑–≤–æ–ª—è—é—â–∏—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–º —Å–∏—Å—Ç–µ–º–∞–º –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –±–µ–∑ —è–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–ª–∞–≥–∞—è—Å—å –Ω–∞ —à–∞–±–ª–æ–Ω—ã –∏ –≤—ã–≤–æ–¥—ã.

**–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:**

- –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö
- –û–±–æ–±—â–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π
- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è

## –¢–∏–ø—ã –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

### 1. –û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (Supervised Learning)

- **–†–µ–≥—Ä–µ—Å—Å–∏—è** - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
- **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è** - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –º–µ—Ç–æ–∫

### 2. –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è (Unsupervised Learning)

- **–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è** - –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤
- **–ê—Å—Å–æ—Ü–∏–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞** - –ø–æ–∏—Å–∫ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π
- **–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏** - —É–ø—Ä–æ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö

### 3. –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Reinforcement Learning)

- –û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å–æ —Å—Ä–µ–¥–æ–π
- –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥ –∏ —à—Ç—Ä–∞—Ñ–æ–≤
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏

## –û—Å–Ω–æ–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

### Scikit-learn —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞:

```python
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
pip install scikit-learn pandas numpy matplotlib seaborn

# –û—Å–Ω–æ–≤–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, mean_squared_error
```

## –†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

### –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è:

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
lr = LinearRegression()
lr.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred = lr.predict(X_test)

# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.2f}")
print(f"R¬≤: {r2:.2f}")
```

### –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
poly_reg = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('linear', LinearRegression())
])

poly_reg.fit(X_train, y_train)
y_pred = poly_reg.predict(X_test)
```

### –†–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:

```python
# Ridge —Ä–µ–≥—Ä–µ—Å—Å–∏—è (L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
from sklearn.linear_model import Ridge
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Lasso —Ä–µ–≥—Ä–µ—Å—Å–∏—è (L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
from sklearn.linear_model import Lasso
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# Elastic Net (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è L1 –∏ L2)
from sklearn.linear_model import ElasticNet
elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic.fit(X_train, y_train)
```

## –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

### –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è:

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred = log_reg.predict(X_test)
y_prob = log_reg.predict_proba(X_test)

# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print(classification_report(y_test, y_pred))
```

### –ú–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ (SVM):

```python
from sklearn.svm import SVC

# –õ–∏–Ω–µ–π–Ω—ã–π SVM
svm_linear = SVC(kernel='linear', random_state=42)
svm_linear.fit(X_train, y_train)

# –ù–µ–ª–∏–Ω–µ–π–Ω—ã–π SVM (RBF —è–¥—Ä–æ)
svm_rbf = SVC(kernel='rbf', gamma='scale', random_state=42)
svm_rbf.fit(X_train, y_train)

# –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–π SVM
svm_poly = SVC(kernel='poly', degree=3, random_state=42)
svm_poly.fit(X_train, y_train)
```

### k-–±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π (KNN):

```python
from sklearn.neighbors import KNeighborsClassifier

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–∞ —Å–æ—Å–µ–¥–µ–π
neighbors_range = range(1, 21)
accuracies = []

for k in neighbors_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    accuracy = knn.score(X_test, y_test)
    accuracies.append(accuracy)

# –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ k
optimal_k = neighbors_range[np.argmax(accuracies)]
print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Å–æ—Å–µ–¥–µ–π: {optimal_k}")

# –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
knn_final = KNeighborsClassifier(n_neighbors=optimal_k)
knn_final.fit(X_train, y_train)
```

### –ù–∞–∏–≤–Ω—ã–π –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä:

```python
from sklearn.naive_bayes import GaussianNB, MultinomialNB

# –ì–∞—É—Å—Å–æ–≤—Å–∫–∏–π –Ω–∞–∏–≤–Ω—ã–π –±–∞–π–µ—Å (–¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
gnb = GaussianNB()
gnb.fit(X_train, y_train)

# –ú—É–ª—å—Ç–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–π –Ω–∞–∏–≤–Ω—ã–π –±–∞–π–µ—Å (–¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
```

### –î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π:

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π
dt = DecisionTreeClassifier(
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42
)
dt.fit(X_train, y_train)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ä–µ–≤–∞
plt.figure(figsize=(20, 10))
plot_tree(dt, filled=True, feature_names=feature_names, class_names=class_names)
plt.show()

# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
feature_importance = dt.feature_importances_
```

## –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è

### K-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è:

```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
inertias = []
silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))

# –ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(K_range, inertias, 'bo-')
plt.xlabel('–ß–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
plt.ylabel('–ò–Ω–µ—Ä—Ü–∏—è')
plt.title('–ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è')

plt.subplot(1, 2, 2)
plt.plot(K_range, silhouette_scores, 'ro-')
plt.xlabel('–ß–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Analysis')
plt.tight_layout()
plt.show()

# –§–∏–Ω–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
optimal_k = 3  # –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
clusters = kmeans_final.fit_predict(X_scaled)
```

### –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è:

```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º—ã
linkage_matrix = linkage(X_scaled, method='ward')
plt.figure(figsize=(12, 8))
dendrogram(linkage_matrix)
plt.title('–î–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏')
plt.xlabel('–û–±—Ä–∞–∑—Ü—ã')
plt.ylabel('–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ')
plt.show()

# –ê–≥–ª–æ–º–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')
agg_clusters = agg_clustering.fit_predict(X_scaled)
```

### DBSCAN (Density-Based Spatial Clustering):

```python
from sklearn.cluster import DBSCAN

# DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_clusters = dbscan.fit_predict(X_scaled)

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–±–µ–∑ —à—É–º–∞)
n_clusters = len(set(dbscan_clusters)) - (1 if -1 in dbscan_clusters else 0)
n_noise = list(dbscan_clusters).count(-1)

print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à—É–º–æ–≤—ã—Ö —Ç–æ—á–µ–∫: {n_noise}")
```

## –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã

### Random Forest:

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42
)
rf.fit(X_train, y_train)

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid_search.fit(X_train, y_train)

best_rf = grid_search.best_estimator_
print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")

# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
feature_importance = best_rf.feature_importances_
```

### Gradient Boosting:

```python
from sklearn.ensemble import GradientBoostingClassifier

# –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
gb = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
gb.fit(X_train, y_train)

# XGBoost (–±–æ–ª–µ–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
import xgboost as xgb

xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
xgb_model.fit(X_train, y_train)

# LightGBM
import lightgbm as lgb

lgb_model = lgb.LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
lgb_model.fit(X_train, y_train)
```

### Voting Classifier:

```python
from sklearn.ensemble import VotingClassifier

# –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
lr = LogisticRegression(random_state=42)
rf = RandomForestClassifier(random_state=42)
svm = SVC(probability=True, random_state=42)

# Soft voting (—Å—Ä–µ–¥–Ω–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π)
voting_soft = VotingClassifier(
    estimators=[('lr', lr), ('rf', rf), ('svm', svm)],
    voting='soft'
)
voting_soft.fit(X_train, y_train)

# Hard voting (–±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –≥–æ–ª–æ—Å–æ–≤)
voting_hard = VotingClassifier(
    estimators=[('lr', lr), ('rf', rf), ('svm', svm)],
    voting='hard'
)
voting_hard.fit(X_train, y_train)
```

## –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ

### TensorFlow/Keras —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
X_train_scaled = StandardScaler().fit_transform(X_train)
X_test_scaled = StandardScaler().fit_transform(X_test)

# –ü—Ä–æ—Å—Ç–∞—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è —Å–µ—Ç—å
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dropout(0.3),
    layers.Dense(32, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')  # –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
])

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
history = model.fit(
    X_train_scaled, y_train,
    batch_size=32,
    epochs=100,
    validation_split=0.2,
    callbacks=[
        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
    ]
)

# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)
print(f"Test Accuracy: {test_accuracy:.4f}")
```

### –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (`CNN`):

```python
# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
model_cnn = keras.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

model_cnn.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

### –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (`RNN/LSTM`):

```python
# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏
model_lstm = keras.Sequential([
    layers.Embedding(vocab_size, 128, input_length=max_length),
    layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    layers.Dense(32, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')
])

model_lstm.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

## –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π

### –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:

```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)

# –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# ROC-AUC (–¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
if len(np.unique(y_test)) == 2:
    auc = roc_auc_score(y_test, y_prob[:, 1])
    print(f"AUC: {auc:.4f}")
```

### –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:

```python
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    mean_squared_log_error
)

# –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"R¬≤: {r2:.4f}")
```

### –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è:

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold

# –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
cv_scores = cross_val_score(
    model, X, y, 
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring='accuracy'
)

print(f"CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
from sklearn.model_selection import cross_validate

scoring = ['accuracy', 'precision_macro', 'recall_macro']
cv_results = cross_validate(model, X, y, cv=5, scoring=scoring)
```

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã

### –ü—Ä–∏–º–µ—Ä ‚Ññ1: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ü–µ–Ω –Ω–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å

```python
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = pd.read_csv('housing_data.csv')

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
X = df[['bedrooms', 'bathrooms', 'sqft_living', 'location_rating']]
y = df['price']

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X_train_scaled, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
predictions = rf_regressor.predict(X_test_scaled)

# –û—Ü–µ–Ω–∫–∞
mse = mean_squared_error(y_test, predictions)
rmse = np.sqrt(mse)
print(f"RMSE: ${rmse:.2f}")
```

### –ü—Ä–∏–º–µ—Ä ‚Ññ2: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ü–∏—Ñ—Ä

```python
from sklearn.datasets import load_digits
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
digits = load_digits()
X, y = digits.data, digits.target

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SVM
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]
}

grid_search = GridSearchCV(
    SVC(kernel='rbf'),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)
best_svm = grid_search.best_estimator_

# –û—Ü–µ–Ω–∫–∞
accuracy = best_svm.score(X_test, y_test)
print(f"Accuracy: {accuracy:.4f}")
print(f"Best parameters: {grid_search.best_params_}")
```

### –ü—Ä–∏–º–µ—Ä ‚Ññ3: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞

```python
# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤
customer_data = pd.DataFrame({
    'annual_spending': np.random.normal(50000, 15000, 1000),
    'frequency_of_purchase': np.random.poisson(12, 1000),
    'average_order_value': np.random.normal(200, 50, 1000)
})

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
scaler = StandardScaler()
customer_scaled = scaler.fit_transform(customer_data)

# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
customer_segments = kmeans.fit_predict(customer_scaled)

# –ê–Ω–∞–ª–∏–∑ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
customer_data['segment'] = customer_segments
segment_analysis = customer_data.groupby('segment').agg({
    'annual_spending': 'mean',
    'frequency_of_purchase': 'mean',
    'average_order_value': 'mean'
})

print("–ê–Ω–∞–ª–∏–∑ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤:")
print(segment_analysis)
```

## –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏

### 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö:

```python
# –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)

# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first', sparse=False)
X_encoded = encoder.fit_transform(categorical_features)

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# StandardScaler –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
scaler_standard = StandardScaler()
X_standard = scaler_standard.fit_transform(X_numeric)

# MinMaxScaler –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
scaler_minmax = MinMaxScaler()
X_minmax = scaler_minmax.fit_transform(X_numeric)
```

### 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:

```python
# –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    stratify=y,  # —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤
    random_state=42
)

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–æ–π
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42
)
```

### 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è:

```python
# –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
from sklearn.linear_model import Ridge, Lasso

# Early stopping –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π
early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

# Dropout –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π
model = keras.Sequential([
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),  # 30% –Ω–µ–π—Ä–æ–Ω–æ–≤ –æ—Ç–∫–ª—é—á–∞—é—Ç—Å—è
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])
```

### 4. Feature Engineering:

```python
# –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, interaction_only=True)
X_poly = poly.fit_transform(X)

# –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(score_func=f_classif, k=10)
X_selected = selector.fit_transform(X, y)

# PCA –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
from sklearn.decomposition import PCA

pca = PCA(n_components=0.95)  # —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å 95% –¥–∏—Å–ø–µ—Ä—Å–∏–∏
X_pca = pca.fit_transform(X_scaled)
```

### 5. Pipeline –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏:

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# –°–æ–∑–¥–∞–Ω–∏–µ pipeline
numeric_features = ['age', 'income', 'score']
categorical_features = ['gender', 'city']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(drop='first'), categorical_features)
    ]
)

pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ pipeline
pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)
```

---

#### üíº –ê–≤—Ç–æ—Ä: –î—É–ø–ª–µ–π –ú–∞–∫—Å–∏–º –ò–≥–æ—Ä–µ–≤–∏—á

### üì≤ –ö–æ–Ω—Ç–∞–∫—Ç—ã:

- **Telegram ‚Ññ1:** [@quadd4rv1n7](https://t.me/quadd4rv1n7)
- **Telegram ‚Ññ2:** [@dupley_maxim_1999](https://t.me/dupley_maxim_1999)

üìÖ **–î–∞—Ç–∞:** 26.01.2026

‚ñ∂Ô∏è –í–µ—Ä—Å–∏—è 1.0

---
> üìß **–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤—É:** maksimqwe42@mail.ru

---

### üíº –ü—Ä–æ—Ñ–∏–ª—å –Ω–∞ Profi.ru
[![Profi.ru Profile](https://img.shields.io/badge/Profi.ru-–î—É–ø–ª–µ–π%20–ú.–ò.-FF6B35?style=for-the-badge)](https://profi.ru/profile/DupleyMI)

> –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –∏ —É—Å–ª—É–≥–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ Profi.ru

---

### üìö –£—Å–ª—É–≥–∏ –æ–±—É—á–µ–Ω–∏—è
[![–û–±—É—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º –∏ —è–∑—ã–∫–∞–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ Kwork](https://img.shields.io/badge/Kwork-–û–±—É—á–µ–Ω–∏–µ%20–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é-blue?style=for-the-badge&logo=kwork)](https://kwork.ru/usability-testing/42465951/–æ–±—É—á–µ–Ω–∏–µ-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º-–∏-—è–∑—ã–∫–∞–º-–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è)

> –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º –∏ —è–∑—ã–∫–∞–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –∏ –∫—É—Ä—Å—ã –æ—Ç –æ–ø—ã—Ç–Ω–æ–≥–æ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è.

---

### üè´ –û —à–∫–æ–ª–µ
[![Website](https://img.shields.io/badge/Maestro7IT-school--maestro7it.ru-darkgreen?style=for-the-badge)](https://school-maestro7it.ru/)

> –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è —à–∫–æ–ª–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∞—è—Å—è –Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.