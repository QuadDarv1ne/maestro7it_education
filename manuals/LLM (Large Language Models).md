# –ü–æ–ª–Ω—ã–π –º–∞–Ω—É–∞–ª –ø–æ –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM): –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–í–≤–µ–¥–µ–Ω–∏–µ –≤ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏](#–≤–≤–µ–¥–µ–Ω–∏–µ-–≤-–±–æ–ª—å—à–∏–µ-—è–∑—ã–∫–æ–≤—ã–µ-–º–æ–¥–µ–ª–∏)
2. [–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã LLM](#–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã-llm)
3. [–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∏ –º–µ—Ö–∞–Ω–∏–∑–º—ã –≤–Ω–∏–º–∞–Ω–∏—è](#—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã-–∏-–º–µ—Ö–∞–Ω–∏–∑–º—ã-–≤–Ω–∏–º–∞–Ω–∏—è)
4. [–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ](#–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ-–æ–±—É—á–µ–Ω–∏–µ)
5. [–¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ (Fine-tuning)](#—Ç–æ–Ω–∫–∞—è-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞-fine-tuning)
6. [–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (LoRA –∏ PEFT)](#–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤-lora-–∏-peft)
7. [–ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è](#–∏–Ω—Ñ–µ—Ä–µ–Ω—Å-–∏-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
8. [–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã](#–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ-–ø—Ä–∏–º–µ—Ä—ã)
9. [–õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏](#–ª—É—á—à–∏–µ-–ø—Ä–∞–∫—Ç–∏–∫–∏)

## –í–≤–µ–¥–µ–Ω–∏–µ –≤ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏

**–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (Large Language Models, LLM)** ‚Äî —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Å –æ–≥—Ä–æ–º–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–º–∏–ª–ª–∏–∞—Ä–¥—ã –∏ —Ç—Ä–∏–ª–ª–∏–æ–Ω—ã), –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –º–∞—Å—Å–∏–≤–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–∞—Ö —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.

**–û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ LLM:**

- –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- `Few-shot` –∏ `zero-shot` –æ–±—É—á–µ–Ω–∏–µ
- –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç—å
- –ú–Ω–æ–≥–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (—Ç–µ–∫—Å—Ç, –∫–æ–¥, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è)

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã LLM

### –û—Å–Ω–æ–≤–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:

#### 1. GPT (Generative Pre-trained Transformer)

```python
# –ü—Ä–∏–º–µ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã GPT-like –º–æ–¥–µ–ª–∏
import torch
import torch.nn as nn
from transformers import GPT2LMHeadModel, GPT2Tokenizer

class CustomGPT(nn.Module):
    def __init__(self, vocab_size, d_model=768, n_heads=12, n_layers=12):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(1024, d_model)
        self.layers = nn.ModuleList([
            nn.TransformerDecoderLayer(
                d_model=d_model,
                nhead=n_heads,
                dim_feedforward=d_model * 4,
                batch_first=True
            ) for _ in range(n_layers)
        ])
        self.lm_head = nn.Linear(d_model, vocab_size)
        
    def forward(self, input_ids, attention_mask=None):
        seq_len = input_ids.size(1)
        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)
        
        x = self.token_embedding(input_ids) + self.position_embedding(positions)
        
        for layer in self.layers:
            x = layer(x, x, tgt_mask=self.generate_causal_mask(seq_len))
            
        return self.lm_head(x)
    
    def generate_causal_mask(self, size):
        mask = torch.triu(torch.ones(size, size), diagonal=1).bool()
        return mask
```

#### 2. BERT (Bidirectional Encoder Representations)

```python
# –ü—Ä–∏–º–µ—Ä BERT-–ø–æ–¥–æ–±–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
class BERTModel(nn.Module):
    def __init__(self, vocab_size, d_model=768, n_heads=12, n_layers=12):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.segment_embedding = nn.Embedding(2, d_model)
        self.position_embedding = nn.Embedding(512, d_model)
        
        self.encoder_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=n_heads,
                dim_feedforward=d_model * 4,
                batch_first=True
            ) for _ in range(n_layers)
        ])
        
        self.pooler = nn.Linear(d_model, d_model)
        self.activation = nn.Tanh()
        
    def forward(self, input_ids, token_type_ids=None, attention_mask=None):
        seq_len = input_ids.size(1)
        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)
        
        embeddings = self.token_embedding(input_ids) + self.position_embedding(positions)
        
        if token_type_ids is not None:
            embeddings += self.segment_embedding(token_type_ids)
            
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–∞—Å–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è
        if attention_mask is not None:
            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
            attention_mask = (1.0 - attention_mask) * -10000.0
            
        for layer in self.encoder_layers:
            embeddings = layer(embeddings, src_key_padding_mask=attention_mask)
            
        # Pooler –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è [CLS] —Ç–æ–∫–µ–Ω–∞
        pooled_output = self.activation(self.pooler(embeddings[:, 0]))
        
        return embeddings, pooled_output
```

#### 3. T5 (Text-to-Text Transfer Transformer)

```python
# –ü—Ä–∏–º–µ—Ä T5-–ø–æ–¥–æ–±–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
class T5Model(nn.Module):
    def __init__(self, vocab_size, d_model=512, n_heads=8, n_layers=6):
        super().__init__()
        self.shared_embedding = nn.Embedding(vocab_size, d_model)
        
        # Encoder
        self.encoder_layers = nn.ModuleList([
            T5EncoderLayer(d_model, n_heads) for _ in range(n_layers)
        ])
        
        # Decoder
        self.decoder_layers = nn.ModuleList([
            T5DecoderLayer(d_model, n_heads) for _ in range(n_layers)
        ])
        
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
        
    def forward(self, input_ids, decoder_input_ids, attention_mask=None):
        # Encoder
        encoder_hidden_states = self.shared_embedding(input_ids)
        for layer in self.encoder_layers:
            encoder_hidden_states = layer(encoder_hidden_states, attention_mask)
            
        # Decoder
        decoder_hidden_states = self.shared_embedding(decoder_input_ids)
        for layer in self.decoder_layers:
            decoder_hidden_states = layer(
                decoder_hidden_states, 
                encoder_hidden_states,
                attention_mask
            )
            
        return self.lm_head(decoder_hidden_states)
```

## –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∏ –º–µ—Ö–∞–Ω–∏–∑–º—ã –≤–Ω–∏–º–∞–Ω–∏—è

### Self-Attention –º–µ—Ö–∞–Ω–∏–∑–º:

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out = nn.Linear(d_model, d_model)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≥–æ–ª–æ–≤—ã
        Q = self.q_linear(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.k_linear(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.v_linear(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
            
        attention = torch.softmax(scores, dim=-1)
        context = torch.matmul(attention, V)
        
        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≥–æ–ª–æ–≤ –∏ –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        output = self.out(context)
        
        return output, attention
```

### Position-wise Feed-Forward Network:

```python
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.GELU()
        
    def forward(self, x):
        return self.w_2(self.dropout(self.activation(self.w_1(x))))
```

## –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

### –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —è–∑—ã–∫–æ–≤–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (MLM):

```python
def mlm_pretraining(model, tokenizer, texts, mlm_probability=0.15):
    """
    –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    """
    model.train()
    total_loss = 0
    
    for batch in texts:
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)
        input_ids = inputs['input_ids']
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–æ–∫
        labels = input_ids.clone()
        probability_matrix = torch.full(labels.shape, mlm_probability)
        
        # –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
        masked_indices = torch.bernoulli(probability_matrix).bool()
        labels[~masked_indices] = -100  # –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –ø–æ—Ç–µ—Ä—å
        
        # –ó–∞–º–µ–Ω–∞ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices
        input_ids[indices_replaced] = tokenizer.mask_token_id
        
        # –ó–∞–º–µ–Ω–∞ —Å–ª—É—á–∞–π–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏
        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced
        random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)
        input_ids[indices_random] = random_words[indices_random]
        
        # Forward pass
        outputs = model(input_ids, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        
        # Backward pass
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
    return total_loss / len(texts)
```

### –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ (Causal LM):

```python
def causal_lm_training(model, tokenizer, texts):
    """
    –û–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
    """
    model.train()
    total_loss = 0
    
    for batch in texts:
        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)
        input_ids = inputs['input_ids']
        
        # –°–¥–≤–∏–≥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç–æ–∫
        labels = input_ids.clone()
        labels[:, :-1] = input_ids[:, 1:]
        labels[:, -1] = -100  # –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω
        
        outputs = model(input_ids, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
    return total_loss / len(texts)
```

## –¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ (Fine-tuning)

### –ü–æ–ª–Ω–∞—è —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
model_name = "gpt2"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
def prepare_dataset(examples):
    texts = examples['text']
    encodings = tokenizer(texts, truncation=True, padding=True, max_length=512)
    return encodings

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
train_dataset = dataset.map(prepare_dataset, batched=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    warmup_steps=100,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    evaluation_strategy="steps",
    eval_steps=1000,
    fp16=True,  # –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ GPU
)

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
)

# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
trainer.train()
```

### –ò–Ω—Å—Ç—Ä—É–∫—Ç-—Ç—é–Ω–∏–Ω–≥:

```python
def format_instruction_prompt(instruction, input_text="", output=""):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ç-—Ç—é–Ω–∏–Ω–≥–∞"""
    if input_text:
        prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n"
    else:
        prompt = f"### Instruction:\n{instruction}\n\n### Response:\n"
    
    if output:
        prompt += f"{output}"
    
    return prompt

# –ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ç-—Ç—é–Ω–∏–Ω–≥–∞
instruction_data = [
    {
        "instruction": "–ü–µ—Ä–µ–≤–µ–¥–∏ —Ç–µ–∫—Å—Ç —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π",
        "input": "Hello, how are you?",
        "output": "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?"
    },
    {
        "instruction": "–û–±–æ–±—â–∏ —Ç–µ–∫—Å—Ç",
        "input": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞...",
        "output": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º —É—á–∏—Ç—å—Å—è –±–µ–∑ —è–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è."
    }
]

# –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
formatted_prompts = [
    format_instruction_prompt(**item) for item in instruction_data
]
```

## –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (LoRA –∏ PEFT)

### LoRA (Low-Rank Adaptation):

```python
from peft import LoraConfig, get_peft_model, TaskType

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
lora_config = LoraConfig(
    r=8,  # —Ä–∞–Ω–≥ –º–∞—Ç—Ä–∏—Ü
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # –∫–∞–∫–∏–µ –≤–µ—Å–∞ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA –∫ –º–æ–¥–µ–ª–∏
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # –ü–æ–∫–∞–∑–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

# –û–±—É—á–µ–Ω–∏–µ —Å LoRA
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
)

trainer.train()
```

### QLoRA (Quantized LoRA):

```python
from peft import prepare_model_for_kbit_training

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è 4-–±–∏—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
model = prepare_model_for_kbit_training(model)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ QLoRA
lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)
```

### –ê–¥–∞–ø—Ç–µ—Ä—ã (Adapter Tuning):

```python
class AdapterLayer(nn.Module):
    def __init__(self, d_model, bottleneck_size=64):
        super().__init__()
        self.down_proj = nn.Linear(d_model, bottleneck_size)
        self.up_proj = nn.Linear(bottleneck_size, d_model)
        self.activation = nn.ReLU()
        self.layer_norm = nn.LayerNorm(d_model)
        
    def forward(self, x):
        residual = x
        x = self.layer_norm(x)
        x = self.down_proj(x)
        x = self.activation(x)
        x = self.up_proj(x)
        return x + residual

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã–º —Å–ª–æ—è–º
def add_adapters_to_model(model, bottleneck_size=64):
    for layer in model.transformer.h:
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ attention –∏ FFN —Å–ª–æ—è
        layer.attn_adapter = AdapterLayer(model.config.n_embd, bottleneck_size)
        layer.mlp_adapter = AdapterLayer(model.config.n_embd, bottleneck_size)
    return model
```

## –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞:

```python
def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.7, top_p=0.9):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
    model.eval()
    
    inputs = tokenizer.encode(prompt, return_tensors='pt')
    
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=max_length,
            temperature=temperature,
            do_sample=True,
            top_p=top_p,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
prompt = "–í –Ω–∞—É–∫–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"
generated = generate_text(model, tokenizer, prompt, max_length=200)
print(generated)
```

### Beam Search –≥–µ–Ω–µ—Ä–∞—Ü–∏—è:

```python
def beam_search_generation(model, tokenizer, prompt, max_length=100, num_beams=5):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å beam search"""
    model.eval()
    
    inputs = tokenizer.encode(prompt, return_tensors='pt')
    
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=max_length,
            num_beams=num_beams,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞:

```python
# –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
from torch.quantization import quantize_dynamic

quantized_model = quantize_dynamic(
    model, {nn.Linear}, dtype=torch.qint8
)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ONNX –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
import torch.onnx

dummy_input = torch.randint(0, 1000, (1, 512))
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={
        'input': {0: 'batch_size', 1: 'sequence'},
        'output': {0: 'batch_size', 1: 'sequence'}
    }
)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ TensorRT (–¥–ª—è NVIDIA GPU)
# –¢—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ tensorrt –∏ onnxruntime-gpu
```

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã

### –ü—Ä–∏–º–µ—Ä ‚Ññ1: –ß–∞—Ç-–±–æ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM

```python
class ChatBot:
    def __init__(self, model_name="gpt2"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.chat_history = []
        
    def get_response(self, user_message, max_length=150):
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.chat_history.append(f"User: {user_message}")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context = "\n".join(self.chat_history[-5:])  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π
        prompt = f"{context}\nAI:"
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        inputs = self.tokenizer.encode(prompt, return_tensors='pt')
        
        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_length=len(inputs[0]) + max_length,
                temperature=0.7,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        ai_response = response[len(prompt):].strip()
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.chat_history.append(f"AI: {ai_response}")
        
        return ai_response

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
chatbot = ChatBot()
response = chatbot.get_response("–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?")
print(response)
```

### –ü—Ä–∏–º–µ—Ä ‚Ññ2: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å LLM

```python
def classify_text_with_llm(model, tokenizer, text, labels):
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM"""
    prompt = f"–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç. –í–æ–∑–º–æ–∂–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {', '.join(labels)}.\n\n–¢–µ–∫—Å—Ç: {text}\n\n–ö–∞—Ç–µ–≥–æ—Ä–∏—è:"
    
    inputs = tokenizer.encode(prompt, return_tensors='pt')
    
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=len(inputs[0]) + 20,
            temperature=0.1,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            pad_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    predicted_label = response[len(prompt):].strip()
    
    return predicted_label

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
labels = ["—Å–ø–æ—Ä—Ç", "–ø–æ–ª–∏—Ç–∏–∫–∞", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–∫—É–ª—å—Ç—É—Ä–∞"]
text = "–ù–æ–≤—ã–π —Å–º–∞—Ä—Ç—Ñ–æ–Ω —Å —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–π –∫–∞–º–µ—Ä–æ–π –±—ã–ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞ –≤—ã—Å—Ç–∞–≤–∫–µ"
predicted = classify_text_with_llm(model, tokenizer, text, labels)
print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {predicted}")
```

### –ü—Ä–∏–º–µ—Ä ‚Ññ3: –†–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞

```python
def summarize_text(model, tokenizer, text, max_summary_length=150):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—é–º–µ —Ç–µ–∫—Å—Ç–∞"""
    prompt = f"–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n\n{text}\n\n–†–µ–∑—é–º–µ:"
    
    inputs = tokenizer.encode(prompt, return_tensors='pt')
    
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=len(inputs[0]) + max_summary_length,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary[len(prompt):].strip()

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
long_text = """
–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, 
–∫–æ—Ç–æ—Ä–∞—è —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π, 
–ø–æ–∑–≤–æ–ª—è—é—â–∏—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–º —Å–∏—Å—Ç–µ–º–∞–º –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –±–µ–∑ —è–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. 
–≠—Ç–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –Ω–∞—Ö–æ–¥–∏—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ñ–µ—Ä–∞—Ö: –æ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ 
–¥–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞.
"""

summary = summarize_text(model, tokenizer, long_text)
print(f"–†–µ–∑—é–º–µ: {summary}")
```

## –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏

### 1. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é:

```python
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ gradient checkpointing
from transformers import TrainingArguments

training_args = TrainingArguments(
    # ... –¥—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    gradient_checkpointing=True,  # –£–º–µ–Ω—å—à–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
)

# –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ GPU
torch.cuda.empty_cache()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ mixed precision training
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    outputs = model(inputs)
    loss = outputs.loss
    
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:

```python
# Sliding window attention –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
def sliding_window_attention(model, tokenizer, long_text, window_size=512, stride=256):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞"""
    tokens = tokenizer.encode(long_text, return_tensors='pt')[0]
    chunks = []
    
    for i in range(0, len(tokens), stride):
        chunk = tokens[i:i + window_size]
        if len(chunk) < window_size:
            chunk = torch.cat([chunk, torch.zeros(window_size - len(chunk), dtype=torch.long)])
        chunks.append(chunk)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
    embeddings = []
    for chunk in chunks:
        with torch.no_grad():
            output = model(chunk.unsqueeze(0), output_hidden_states=True)
            embedding = output.hidden_states[-1][:, 0, :]  # [CLS] —Ç–æ–∫–µ–Ω
            embeddings.append(embedding)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    final_embedding = torch.mean(torch.stack(embeddings), dim=0)
    return final_embedding
```

### 3. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ—Ç–ª–∞–¥–∫–∞:

```python
# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
import wandb
from transformers import TrainerCallback

class LoggingCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if state.is_local_process_zero:
            wandb.log(logs)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    callbacks=[LoggingCallback()],
)

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
def monitor_gradients(model):
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–æ—Ä–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤"""
    total_norm = 0
    for name, param in model.named_parameters():
        if param.grad is not None:
            param_norm = param.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    total_norm = total_norm ** (1. / 2)
    return total_norm
```

### 4. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:

```python
from ray import tune
from ray.tune.schedulers import ASHAScheduler

def train_model(config):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    model = AutoModelForCausalLM.from_pretrained("gpt2")
    
    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=config["epochs"],
        per_device_train_batch_size=config["batch_size"],
        learning_rate=config["learning_rate"],
        weight_decay=config["weight_decay"],
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
    )
    
    result = trainer.train()
    return {"loss": result.training_loss}

# –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
config = {
    "epochs": tune.choice([2, 3, 4]),
    "batch_size": tune.choice([2, 4, 8]),
    "learning_rate": tune.loguniform(1e-5, 1e-3),
    "weight_decay": tune.uniform(0.0, 0.3),
}

scheduler = ASHAScheduler(
    metric="loss",
    mode="min",
    max_t=10,
    grace_period=1,
    reduction_factor=2
)

analysis = tune.run(
    train_model,
    resources_per_trial={"cpu": 2, "gpu": 1},
    config=config,
    num_samples=10,
    scheduler=scheduler
)

print("–õ—É—á—à–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:", analysis.best_config)
```

---

#### üíº –ê–≤—Ç–æ—Ä: –î—É–ø–ª–µ–π –ú–∞–∫—Å–∏–º –ò–≥–æ—Ä–µ–≤–∏—á

### üì≤ –ö–æ–Ω—Ç–∞–∫—Ç—ã:

- **Telegram ‚Ññ1:** [@quadd4rv1n7](https://t.me/quadd4rv1n7)
- **Telegram ‚Ññ2:** [@dupley_maxim_1999](https://t.me/dupley_maxim_1999)

üìÖ **–î–∞—Ç–∞:** 26.01.2026

‚ñ∂Ô∏è –í–µ—Ä—Å–∏—è 1.0

---
> üìß **–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤—É:** maksimqwe42@mail.ru

---

### üíº –ü—Ä–æ—Ñ–∏–ª—å –Ω–∞ Profi.ru
[![Profi.ru Profile](https://img.shields.io/badge/Profi.ru-–î—É–ø–ª–µ–π%20–ú.–ò.-FF6B35?style=for-the-badge)](https://profi.ru/profile/DupleyMI)

> –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –∏ —É—Å–ª—É–≥–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ Profi.ru

---

### üìö –£—Å–ª—É–≥–∏ –æ–±—É—á–µ–Ω–∏—è
[![–û–±—É—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º –∏ —è–∑—ã–∫–∞–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ Kwork](https://img.shields.io/badge/Kwork-–û–±—É—á–µ–Ω–∏–µ%20–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é-blue?style=for-the-badge&logo=kwork)](https://kwork.ru/usability-testing/42465951/–æ–±—É—á–µ–Ω–∏–µ-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º-–∏-—è–∑—ã–∫–∞–º-–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è)

> –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º –∏ —è–∑—ã–∫–∞–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –∏ –∫—É—Ä—Å—ã –æ—Ç –æ–ø—ã—Ç–Ω–æ–≥–æ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è.

---

### üè´ –û —à–∫–æ–ª–µ
[![Website](https://img.shields.io/badge/Maestro7IT-school--maestro7it.ru-darkgreen?style=for-the-badge)](https://school-maestro7it.ru/)

> –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è —à–∫–æ–ª–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∞—è—Å—è –Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.