# –†–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–í–≤–µ–¥–µ–Ω–∏–µ –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏](#–≤–≤–µ–¥–µ–Ω–∏–µ-–≤-–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ-—Å–µ—Ç–∏)
2. [–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä](#–æ—Å–Ω–æ–≤–Ω—ã–µ-—Ç–∏–ø—ã-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä)
3. [–°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (CNN)](#—Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ-–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ-—Å–µ—Ç–∏-cnn)
4. [–†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (RNN)](#—Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ-–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ-—Å–µ—Ç–∏-rnn)
5. [–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã](#—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã)
6. [–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏](#–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ-–º–æ–¥–µ–ª–∏)
7. [–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã](#—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)
8. [–í—ã–±–æ—Ä –ø–æ–¥—Ö–æ–¥—è—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã](#–≤—ã–±–æ—Ä-–ø–æ–¥—Ö–æ–¥—è—â–µ–π-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)
9. [–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã](#–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ-–ø—Ä–∏–º–µ—Ä—ã)

## –í–≤–µ–¥–µ–Ω–∏–µ –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏

**–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å** ‚Äî —ç—Ç–æ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –º–æ–∑–≥–∞.

–û–Ω–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö —É–∑–ª–æ–≤ (–Ω–µ–π—Ä–æ–Ω–æ–≤), –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –ø–µ—Ä–µ–¥–∞—é—Ç —Å–∏–≥–Ω–∞–ª—ã –¥—Ä—É–≥ –¥—Ä—É–≥—É.

### –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
- **–ù–µ–π—Ä–æ–Ω (–ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω)**: –ë–∞–∑–æ–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–µ—Ç–∏
- **–í–µ—Å–∞**: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–µ —Å–∏–ª—É —Å–≤—è–∑–µ–π
- **–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏**: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—ã—Ö–æ–¥ –Ω–µ–π—Ä–æ–Ω–∞
- **–°–ª–æ–∏**: –ì—Ä—É–ø–ø—ã –Ω–µ–π—Ä–æ–Ω–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é

### –¢–∏–ø—ã –∑–∞–¥–∞—á:
- **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–∫–æ—à–∫–∞/—Å–æ–±–∞–∫–∞)
- **–†–µ–≥—Ä–µ—Å—Å–∏—è**: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
- **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è**: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è**: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤

---

## –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä

### 1. –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–µ—Ç–∏ (Feedforward/MLP)

**–û–ø–∏—Å–∞–Ω–∏–µ**: –ü—Ä–æ—Å—Ç–µ–π—à–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –≥–¥–µ –∫–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω —Å–æ–µ–¥–∏–Ω–µ–Ω —Å–æ –≤—Å–µ–º–∏ –Ω–µ–π—Ä–æ–Ω–∞–º–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ—è.

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞**:
- –ü—Ä–æ—Å—Ç–æ—Ç–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
- –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç —Å —Ç–∞–±–ª–∏—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏**:
- –ù–µ —É—á–∏—Ç—ã–≤–∞—é—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏
- –¢—Ä–µ–±—É—é—Ç –º–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ü–ª–æ—Ö–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—Ç—Å—è

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
- –ë–∞–∑–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

```python
import tensorflow as tf
from tensorflow.keras import layers, models

# –ü—Ä–∏–º–µ—Ä MLP –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
model = models.Sequential([
    layers.Dense(128, activation='relu', input_shape=(784,)),
    layers.Dropout(0.2),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10, activation='softmax')
])
```

---

## –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (CNN)

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ CNN

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏**:
- **–°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏**: –í—ã–¥–µ–ª—è—é—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
- **–ü—É–ª–∏–Ω–≥**: –°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
- **–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏**: –§–∏–Ω–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è

### –°–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π (Convolutional Layer)

```python
# –°–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π –≤ TensorFlow/Keras
layers.Conv2D(
    filters=32,           # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤
    kernel_size=(3, 3),   # –†–∞–∑–º–µ—Ä —è–¥—Ä–∞
    activation='relu',
    input_shape=(28, 28, 1)
)
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã**:
- `filters`: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
- `kernel_size`: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ —Å–≤–µ—Ä—Ç–∫–∏
- `stride`: –®–∞–≥ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è —è–¥—Ä–∞
- `padding`: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü (same/valid)

### –ü—É–ª–∏–Ω–≥ (Pooling)

```python
# MaxPooling
layers.MaxPooling2D(pool_size=(2, 2))

# AveragePooling
layers.AveragePooling2D(pool_size=(2, 2))
```

### –ü–æ–ª–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ CNN

```python
# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (MNIST)
model = models.Sequential([
    # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –±–ª–æ–∫–∏
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    
    layers.Conv2D(64, (3, 3), activation='relu'),
    
    # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è —á–∞—Å—Ç—å
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])
```

### –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã CNN

#### LeNet-5 (1998)
```python
# –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π LeNet
model = models.Sequential([
    layers.Conv2D(6, (5, 5), activation='tanh', input_shape=(32, 32, 1)),
    layers.AveragePooling2D((2, 2)),
    layers.Conv2D(16, (5, 5), activation='tanh'),
    layers.AveragePooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(120, activation='tanh'),
    layers.Dense(84, activation='tanh'),
    layers.Dense(10, activation='softmax')
])
```

#### VGG (2014)
```python
# VGG-16 (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
def create_vgg_block(n_filters, n_layers):
    block = []
    for _ in range(n_layers):
        block.append(layers.Conv2D(n_filters, (3, 3), activation='relu', padding='same'))
    block.append(layers.MaxPooling2D((2, 2)))
    return block

# –ë–ª–æ–∫–∏ VGG
model = models.Sequential([
    *create_vgg_block(64, 2),
    *create_vgg_block(128, 2),
    *create_vgg_block(256, 3),
    *create_vgg_block(512, 3),
    *create_vgg_block(512, 3),
    layers.Flatten(),
    layers.Dense(4096, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(4096, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1000, activation='softmax')
])
```

#### ResNet (2015)
```python
# Residual Block
def residual_block(x, filters, stride=1):
    shortcut = x
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –ø—É—Ç—å
    x = layers.Conv2D(filters, (3, 3), strides=stride, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    
    x = layers.Conv2D(filters, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    
    # Shortcut connection
    if stride != 1 or shortcut.shape[-1] != filters:
        shortcut = layers.Conv2D(filters, (1, 1), strides=stride)(shortcut)
        shortcut = layers.BatchNormalization()(shortcut)
    
    x = layers.Add()([x, shortcut])
    x = layers.ReLU()(x)
    return x
```

### –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è CNN:
- –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤
- –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
- –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏

---

## –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (RNN)

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ RNN

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏**:
- –ò–º–µ—é—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –ø–∞–º—è—Ç—å (—Å–æ—Å—Ç–æ—è–Ω–∏–µ)
- –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã
- –í—ã—Ö–æ–¥ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –≤—Ö–æ–¥–æ–≤

### –ë–∞–∑–æ–≤–∞—è RNN-—è—á–µ–π–∫–∞

```python
# –ü—Ä–æ—Å—Ç–∞—è RNN-—è—á–µ–π–∫–∞
class SimpleRNNCell(tf.keras.layers.Layer):
    def __init__(self, units):
        super().__init__()
        self.units = units
        self.W_hh = self.add_weight(shape=(units, units), initializer='random_normal')
        self.W_xh = self.add_weight(shape=(None, units), initializer='random_normal')
        self.b_h = self.add_weight(shape=(units,), initializer='zeros')
        
    def call(self, x, state):
        h = tf.tanh(tf.matmul(x, self.W_xh) + tf.matmul(state, self.W_hh) + self.b_h)
        return h, h
```

### –ü—Ä–æ–±–ª–µ–º—ã –±–∞–∑–æ–≤—ã—Ö RNN:
- **Vanishing Gradient**: –¢—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –æ–±—É—á–µ–Ω–∏–µ–º –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º
- **Exploding Gradient**: –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–º–∏

### LSTM (Long Short-Term Memory)

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**:
1. **Forget Gate**: –†–µ—à–∞–µ—Ç, —á—Ç–æ –∑–∞–±—ã—Ç—å
2. **Input Gate**: –†–µ—à–∞–µ—Ç, —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å
3. **Output Gate**: –†–µ—à–∞–µ—Ç, —á—Ç–æ –≤—ã–≤–µ—Å—Ç–∏

```python
# LSTM –≤ Keras
model = models.Sequential([
    layers.Embedding(vocab_size, 128, input_length=max_length),
    layers.LSTM(64, return_sequences=True),  # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    layers.LSTM(32),                          # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    layers.Dense(1, activation='sigmoid')
])

# Bidirectional LSTM
model = models.Sequential([
    layers.Embedding(vocab_size, 128, input_length=max_length),
    layers.Bidirectional(layers.LSTM(64)),
    layers.Dense(1, activation='sigmoid')
])
```

### GRU (Gated Recurrent Unit)

**–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è LSTM**:
- –ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ß–∞—Å—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–∞–∫ –∂–µ —Ö–æ—Ä–æ—à–æ, –∫–∞–∫ LSTM

```python
# GRU
model = models.Sequential([
    layers.Embedding(vocab_size, 128, input_length=max_length),
    layers.GRU(64, return_sequences=True),
    layers.GRU(32),
    layers.Dense(1, activation='sigmoid')
])
```

### –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è RNN:
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞
- –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
- –ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥

---

## –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã

### –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è (Attention)

**Self-Attention**:
```python
# –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Self-Attention
class SelfAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = layers.Dense(d_model)
        self.W_k = layers.Dense(d_model)
        self.W_v = layers.Dense(d_model)
        self.W_o = layers.Dense(d_model)
        
    def call(self, x, mask=None):
        batch_size = tf.shape(x)[0]
        
        # –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        Q = self.W_q(x)  # Query
        K = self.W_k(x)  # Key
        V = self.W_v(x)  # Value
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≥–æ–ª–æ–≤—ã
        Q = self.split_heads(Q, batch_size)
        K = self.split_heads(K, batch_size)
        V = self.split_heads(V, batch_size)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è
        attention, weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≥–æ–ª–æ–≤
        attention = tf.transpose(attention, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(attention, (batch_size, -1, self.d_model))
        
        # –õ–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–∞
        output = self.W_o(concat_attention)
        
        return output, weights
```

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Transformer

```python
# Transformer Encoder Layer
class TransformerEncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):
        super().__init__()
        
        self.attention = SelfAttention(d_model, num_heads)
        self.ffn = self.point_wise_feed_forward_network(d_model, dff)
        
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        
        self.dropout1 = layers.Dropout(dropout_rate)
        self.dropout2 = layers.Dropout(dropout_rate)
        
    def point_wise_feed_forward_network(self, d_model, dff):
        return tf.keras.Sequential([
            layers.Dense(dff, activation='relu'),
            layers.Dense(d_model)
        ])
        
    def call(self, x, training, mask=None):
        attn_output, _ = self.attention(x, mask)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)
        
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)
        
        return out2
```

### –ü–æ–ª–Ω—ã–π Transformer

```python
# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
class TransformerClassifier(tf.keras.Model):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, 
                 maximum_position_encoding, dropout_rate=0.1):
        super().__init__()
        
        self.d_model = d_model
        self.num_layers = num_layers
        
        self.embedding = layers.Embedding(input_vocab_size, d_model)
        self.pos_encoding = self.positional_encoding(maximum_position_encoding, d_model)
        
        self.enc_layers = [TransformerEncoderLayer(d_model, num_heads, dff, dropout_rate) 
                          for _ in range(num_layers)]
        
        self.dropout = layers.Dropout(dropout_rate)
        self.final_layer = layers.Dense(1, activation='sigmoid')
        
    def positional_encoding(self, position, d_model):
        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],
                                   np.arange(d_model)[np.newaxis, :],
                                   d_model)
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–∏–Ω—É—Å–∞ –∫ —á–µ—Ç–Ω—ã–º –∏–Ω–¥–µ–∫—Å–∞–º
        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–∞ –∫ –Ω–µ—á–µ—Ç–Ω—ã–º –∏–Ω–¥–µ–∫—Å–∞–º
        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
        
        pos_encoding = angle_rads[np.newaxis, ...]
        
        return tf.cast(pos_encoding, dtype=tf.float32)
        
    def get_angles(self, pos, i, d_model):
        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))
        return pos * angle_rates
        
    def call(self, x, training, mask=None):
        seq_len = tf.shape(x)[1]
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        x = self.embedding(x)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]
        
        x = self.dropout(x, training=training)
        
        for i in range(self.num_layers):
            x = self.enc_layers[i](x, training, mask)
        
        # –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ø—É–ª–ª–∏–Ω–≥
        x = tf.reduce_mean(x, axis=1)
        x = self.final_layer(x)
        
        return x
```

### –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ Transformer:

#### BERT (Bidirectional Encoder Representations)
```python
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏ BERT
from transformers import TFBertForSequenceClassification, BertTokenizer

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
text = "This is a sample text"
inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True)
outputs = model(inputs)
```

#### GPT (Generative Pre-trained Transformer)
```python
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPT
from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
input_text = "The future of AI"
input_ids = tokenizer.encode(input_text, return_tensors='tf')

output = model.generate(
    input_ids,
    max_length=100,
    num_return_sequences=1,
    temperature=0.7,
    do_sample=True
)

generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
```

### –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è Transformer:
- –ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
- –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
- –û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
- –†–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ

---

## –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏

### 1. GAN (Generative Adversarial Networks)

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**:
- **Generator**: –°–æ–∑–¥–∞–µ—Ç –ø–æ–¥–¥–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
- **Discriminator**: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ vs –ø–æ–¥–¥–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

```python
# –ü—Ä–æ—Å—Ç–æ–π GAN –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
def build_generator():
    model = models.Sequential([
        layers.Dense(128 * 7 * 7, activation='relu', input_shape=(100,)),
        layers.Reshape((7, 7, 128)),
        layers.UpSampling2D(),
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.UpSampling2D(),
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.Conv2D(1, (3, 3), activation='tanh', padding='same')
    ])
    return model

def build_discriminator():
    model = models.Sequential([
        layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(1, activation='sigmoid')
    ])
    return model

# –û–±—É—á–µ–Ω–∏–µ GAN
generator = build_generator()
discriminator = build_discriminator()

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞
discriminator.compile(optimizer='adam', loss='binary_crossentropy')

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
discriminator.trainable = False
gan_input = layers.Input(shape=(100,))
x = generator(gan_input)
gan_output = discriminator(x)
gan = models.Model(gan_input, gan_output)
gan.compile(optimizer='adam', loss='binary_crossentropy')
```

### 2. VAE (Variational Autoencoder)

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**:
- **Encoder**: –ö–æ–¥–∏—Ä—É–µ—Ç –≤—Ö–æ–¥ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
- **Decoder**: –î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –∏–∑ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤ –≤—ã—Ö–æ–¥

```python
# VAE –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
class VAE(tf.keras.Model):
    def __init__(self, latent_dim):
        super().__init__()
        self.latent_dim = latent_dim
        self.encoder = self.build_encoder()
        self.decoder = self.build_decoder()
        
    def build_encoder(self):
        encoder_inputs = layers.Input(shape=(28, 28, 1))
        x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(encoder_inputs)
        x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)
        x = layers.Flatten()(x)
        x = layers.Dense(16, activation='relu')(x)
        
        z_mean = layers.Dense(self.latent_dim, name='z_mean')(x)
        z_log_var = layers.Dense(self.latent_dim, name='z_log_var')(x)
        
        return tf.keras.Model(encoder_inputs, [z_mean, z_log_var], name='encoder')
    
    def build_decoder(self):
        latent_inputs = layers.Input(shape=(self.latent_dim,))
        x = layers.Dense(7 * 7 * 64, activation='relu')(latent_inputs)
        x = layers.Reshape((7, 7, 64))(x)
        x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)
        x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)
        decoder_outputs = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)
        
        return tf.keras.Model(latent_inputs, decoder_outputs, name='decoder')
    
    def sample(self, args):
        z_mean, z_log_var = args
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
    def call(self, inputs):
        z_mean, z_log_var = self.encoder(inputs)
        z = self.sample([z_mean, z_log_var])
        reconstructed = self.decoder(z)
        return reconstructed
```

### 3. Diffusion Models

```python
# –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Diffusion Model
class DiffusionModel:
    def __init__(self, timesteps=1000):
        self.timesteps = timesteps
        self.betas = self.cosine_beta_schedule(timesteps)
        self.alphas = 1. - self.betas
        self.alphas_cumprod = np.cumprod(self.alphas, axis=0)
        
    def cosine_beta_schedule(self, timesteps, s=0.008):
        steps = timesteps + 1
        x = np.linspace(0, steps, steps)
        alphas_cumprod = np.cos(((x / steps) + s) / (1 + s) * np.pi * 0.5) ** 2
        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
        return np.clip(betas, 0, 0.999)
    
    def forward_diffusion(self, x_0, t, noise=None):
        if noise is None:
            noise = tf.random.normal(shape=tf.shape(x_0))
        sqrt_alphas_cumprod_t = self.alphas_cumprod[t] ** 0.5
        sqrt_one_minus_alphas_cumprod_t = (1 - self.alphas_cumprod[t]) ** 0.5
        return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise
```

### –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:

- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- –°—Ç–∏–ª–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- –°—É–ø–µ—Ä-—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
- –°–æ–∑–¥–∞–Ω–∏–µ –º—É–∑—ã–∫–∏

---

## –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

### 1. Autoencoders

```python
# –ü—Ä–æ—Å—Ç–æ–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä
class Autoencoder(tf.keras.Model):
    def __init__(self, encoding_dim):
        super().__init__()
        self.encoding_dim = encoding_dim
        self.encoder = self.build_encoder()
        self.decoder = self.build_decoder()
        
    def build_encoder(self):
        return tf.keras.Sequential([
            layers.Flatten(),
            layers.Dense(128, activation='relu'),
            layers.Dense(64, activation='relu'),
            layers.Dense(self.encoding_dim, activation='relu')
        ])
        
    def build_decoder(self):
        return tf.keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.Dense(128, activation='relu'),
            layers.Dense(784, activation='sigmoid'),
            layers.Reshape((28, 28))
        ])
        
    def call(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded
```

### 2. Siamese Networks

```python
# Siamese Network –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
def create_siamese_network(input_shape):
    input_a = layers.Input(shape=input_shape)
    input_b = layers.Input(shape=input_shape)
    
    # –û–±—â–∞—è —Å–µ—Ç—å
    base_network = tf.keras.Sequential([
        layers.Conv2D(64, (10, 10), activation='relu'),
        layers.MaxPooling2D(),
        layers.Conv2D(128, (7, 7), activation='relu'),
        layers.MaxPooling2D(),
        layers.Conv2D(128, (4, 4), activation='relu'),
        layers.MaxPooling2D(),
        layers.Conv2D(256, (4, 4), activation='relu'),
        layers.Flatten(),
        layers.Dense(4096, activation='sigmoid')
    ])
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
    encoded_a = base_network(input_a)
    encoded_b = base_network(input_b)
    
    # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏
    distance = layers.Lambda(
        lambda tensors: tf.abs(tensors[0] - tensors[1])
    )([encoded_a, encoded_b])
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    output = layers.Dense(1, activation='sigmoid')(distance)
    
    return tf.keras.Model([input_a, input_b], output)
```

### 3. Graph Neural Networks (GNN)

```python
# –ü—Ä–æ—Å—Ç–æ–π GNN —Å–ª–æ–π
class GraphConvolution(layers.Layer):
    def __init__(self, output_dim, activation='relu'):
        super().__init__()
        self.output_dim = output_dim
        self.activation = tf.keras.activations.get(activation)
        
    def build(self, input_shape):
        self.W = self.add_weight(
            shape=(input_shape[0][-1], self.output_dim),
            initializer='random_normal',
            trainable=True
        )
        self.b = self.add_weight(
            shape=(self.output_dim,),
            initializer='zeros',
            trainable=True
        )
        
    def call(self, inputs):
        x, adj = inputs
        # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
        support = tf.matmul(x, self.W)
        output = tf.matmul(adj, support) + self.b
        return self.activation(output)
```

### 4. Neural Architecture Search (NAS)

```python
# –ü—Ä–∏–º–µ—Ä NAS —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import tensorflow as tf
from tensorflow.keras import layers
import kerastuner as kt

def build_model(hp):
    model = tf.keras.Sequential()
    model.add(layers.Flatten(input_shape=(28, 28)))
    
    # –ü–æ–∏—Å–∫ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–µ–≤
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(
            units=hp.Int('units_' + str(i), 32, 512, 32),
            activation='relu'
        ))
        model.add(layers.Dropout(hp.Float('dropout_' + str(i), 0, 0.5, 0.1)))
    
    model.add(layers.Dense(10, activation='softmax'))
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model
```

---

## –í—ã–±–æ—Ä –ø–æ–¥—Ö–æ–¥—è—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

### –ö—Ä–∏—Ç–µ—Ä–∏–∏ –≤—ã–±–æ—Ä–∞:

1. **–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö**:
   - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ‚Üí CNN
   - –¢–µ–∫—Å—Ç ‚Üí RNN/LSTM/Transformer
   - –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ‚Üí MLP
   - –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ‚Üí RNN/Transformer

2. **–ó–∞–¥–∞—á–∞**:
   - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è ‚Üí CNN/RNN/Transformer
   - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ‚Üí GAN/VAE/Diffusion
   - –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è ‚Üí Autoencoder
   - –†–µ–≥—Ä–µ—Å—Å–∏—è ‚Üí MLP/CNN

3. **–†–µ—Å—É—Ä—Å—ã**:
   - –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è –º–æ—â–Ω–æ—Å—Ç—å
   - –ü–∞–º—è—Ç—å
   - –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è

### –¢–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:

| –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ | –õ—É—á—à–µ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ | –°–ª–æ–∂–Ω–æ—Å—Ç—å | –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã |
|-------------|-------------------|-----------|----------------------|
| MLP         | –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ | –ù–∏–∑–∫–∞—è | –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ |
| CNN         | –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω–∏–µ |
| RNN/LSTM    | –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ | –°—Ä–µ–¥–Ω—è—è | –í—ã—Å–æ–∫–∏–µ |
| Transformer | NLP/Sequence | –í—ã—Å–æ–∫–∞—è | –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–µ |
| GAN         | –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö | –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è | –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–µ |

---

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã

### 1. –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (CNN)

```python
import tensorflow as tf
from tensorflow.keras import datasets, layers, models

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö MNIST
(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# –û–±—É—á–µ–Ω–∏–µ
model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))

# –û—Ü–µ–Ω–∫–∞
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f'–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {test_acc:.4f}')
```

### 2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (Transformer)

```python
from transformers import TFBertForSequenceClassification, BertTokenizer
import tensorflow as tf

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞
texts = ["I love this movie", "This film is terrible"]
labels = [1, 0]  # 1 = –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π, 0 = –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
encoded = tokenizer(
    texts,
    padding=True,
    truncation=True,
    return_tensors='tf',
    max_length=128
)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
dataset = tf.data.Dataset.from_tensor_slices((dict(encoded), labels))

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

# –û–±—É—á–µ–Ω–∏–µ
model.fit(dataset.batch(8), epochs=3)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
prediction = model.predict(encoded)
print("–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏:", tf.nn.softmax(prediction.logits).numpy()[0][1])
```

### 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (VAE)

```python
# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ VAE –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ü–∏—Ñ—Ä
import tensorflow as tf
import numpy as np

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
latent_dim = 2

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —ç–Ω–∫–æ–¥–µ—Ä–∞
def build_encoder():
    model = tf.keras.Sequential([
        layers.InputLayer(input_shape=(28, 28, 1)),
        layers.Conv2D(32, 3, activation='relu', strides=2, padding='same'),
        layers.Conv2D(64, 3, activation='relu', strides=2, padding='same'),
        layers.Flatten(),
        layers.Dense(16, activation='relu'),
        layers.Dense(latent_dim + latent_dim)
    ])
    return model

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–µ–∫–æ–¥–µ—Ä–∞
def build_decoder():
    model = tf.keras.Sequential([
        layers.InputLayer(input_shape=(latent_dim,)),
        layers.Dense(7 * 7 * 32, activation='relu'),
        layers.Reshape(target_shape=(7, 7, 32)),
        layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same'),
        layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same'),
        layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')
    ])
    return model

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
encoder = build_encoder()
decoder = build_decoder()

# –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
x_train = x_train.astype('float32') / 255.0
x_train = x_train.reshape(-1, 28, 28, 1)

# –ö–∞—Å—Ç–æ–º–Ω—ã–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ü–∏–∫–ª VAE
# ... (–∫–æ–¥ –æ–ø—É—â–µ–Ω –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
latent_sample = tf.random.normal(shape=(1, latent_dim))
generated_image = decoder(latent_sample)
```

### 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (LSTM)

```python
# –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞ —Å LSTM
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# –¢–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
text = """–ü–æ–≥—Ä—É–∂–∞–µ–º—Å—è –≤ –∏–∑—É—á–µ–Ω–∏–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–π –ª–µ–∫—Å–∏–∫–∏.\n 
Computer: [–ö–∞–º—å–ø–∏—é- —Ç–µ—Ä—Ä]:   noun     –∏—Å—Å–∫—É—Å—Å—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ö–∞–∑–µ —ã–≤–∞–∏—Ç—é–∫—Å—Ä–æ –Ω–∞—Ä–æ –∑–æ–≥–∞–∫–µ–º
       (–û–±–∞ –∑–Ω–∞–Ω–∏—é  –µ—Å—Ç—å  –¥–ª—è  —Ä–∞–±–æ—Ç–∞ –Ω–æ—Ç–Ω—ã—Ö —Ä–∞–∑  –¥–∞ –∫–∞–∫ –µ—Å—Ç—å  –∂–æ—Å–Ω–æ–≥–æ
 —Ä—Ä–∑–≤—Ç–∏–∞–Ω–µ–Ω  –∞–≤—Ç–æ–º–∞—Ç).   1"""

tokenizer = Tokenizer()
text_data = [
    "the quick brown fox jumps over the lazy dog",
    "machine learning is a subset of artificial intelligence",
    "neural networks are inspired by the human brain"
]

tokenizer.fit_on_texts(text_data)
total_words = len(tokenizer.word_index) + 1

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
input_sequences = []
for line in text_data:
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i+1]
        input_sequences.append(n_gram_sequence)

# –ü–∞–¥–¥–∏–Ω–≥
max_sequence_len = max([len(x) for x in input_sequences])
input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
xs, labels = input_sequences[:,:-1], input_sequences[:,-1]
ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(total_words, 100, input_length=max_sequence_len-1),
    tf.keras.layers.LSTM(150, return_sequences=True),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(100),
    tf.keras.layers.Dense(total_words, activation='softmax')
])

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# –û–±—É—á–µ–Ω–∏–µ
model.fit(xs, ys, epochs=100, verbose=1)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len):
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
        predicted = model.predict(token_list, verbose=0)
        predicted_id = np.argmax(predicted)
        
        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted_id:
                output_word = word
                break
        seed_text += " " + output_word
    return seed_text

# –ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
generated = generate_text("machine learning", 5, model, tokenizer, max_sequence_len)
print("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:", generated)
```

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

**–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏** ‚Äî —ç—Ç–æ –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –∑–∞–¥–∞—á.

–í—ã–±–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö, –∑–∞–¥–∞—á–∏ –∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. 

### –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:
1. **CNN** ‚Äî –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
2. **RNN/LSTM** ‚Äî –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
3. **Transformer** ‚Äî –¥–ª—è NLP –∏ —Å–ª–æ–∂–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
4. **GAN/VAE** ‚Äî –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
5. **MLP** ‚Äî –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
- –ù–∞—á–∏–Ω–∞–π—Ç–µ —Å –ø—Ä–æ—Å—Ç—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
- –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
- –°–ª–µ–¥–∏—Ç–µ –∑–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é

–≠—Ç–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.

–î–ª—è –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è –∫–∞–∂–¥–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏–∑—É—á–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—É –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ä–∞–±–æ—Ç—ã.

---

#### üíº –ê–≤—Ç–æ—Ä: –î—É–ø–ª–µ–π –ú–∞–∫—Å–∏–º –ò–≥–æ—Ä–µ–≤–∏—á

### üì≤ –ö–æ–Ω—Ç–∞–∫—Ç—ã:

- **Telegram ‚Ññ1:** [@quadd4rv1n7](https://t.me/quadd4rv1n7)
- **Telegram ‚Ññ2:** [@dupley_maxim_1999](https://t.me/dupley_maxim_1999)

üìÖ **–î–∞—Ç–∞:** 26.01.2026

‚ñ∂Ô∏è –í–µ—Ä—Å–∏—è 1.0

---
> üìß **–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤—É:** maksimqwe42@mail.ru

---

### üíº –ü—Ä–æ—Ñ–∏–ª—å –Ω–∞ Profi.ru
[![Profi.ru Profile](https://img.shields.io/badge/Profi.ru-–î—É–ø–ª–µ–π%20–ú.–ò.-FF6B35?style=for-the-badge)](https://profi.ru/profile/DupleyMI)

> –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –∏ —É—Å–ª—É–≥–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ Profi.ru

---

### üìö –£—Å–ª—É–≥–∏ –æ–±—É—á–µ–Ω–∏—è
[![–û–±—É—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º –∏ —è–∑—ã–∫–∞–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ Kwork](https://img.shields.io/badge/Kwork-–û–±—É—á–µ–Ω–∏–µ%20–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é-blue?style=for-the-badge&logo=kwork)](https://kwork.ru/usability-testing/42465951/–æ–±—É—á–µ–Ω–∏–µ-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º-–∏-—è–∑—ã–∫–∞–º-–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è)

> –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º –∏ —è–∑—ã–∫–∞–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –∏ –∫—É—Ä—Å—ã –æ—Ç –æ–ø—ã—Ç–Ω–æ–≥–æ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è.

---

### üè´ –û —à–∫–æ–ª–µ
[![Website](https://img.shields.io/badge/Maestro7IT-school--maestro7it.ru-darkgreen?style=for-the-badge)](https://school-maestro7it.ru/)

> –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è —à–∫–æ–ª–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∞—è—Å—è –Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.
