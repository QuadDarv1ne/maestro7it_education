# üîç –ü–æ–ª–Ω—ã–π –º–∞–Ω—É–∞–ª –ø–æ RAG (Retrieval-Augmented Generation): –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–í–≤–µ–¥–µ–Ω–∏–µ –≤ RAG](#–≤–≤–µ–¥–µ–Ω–∏–µ-–≤-rag)
2. [–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ RAG —Å–∏—Å—Ç–µ–º](#–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞-rag-—Å–∏—Å—Ç–µ–º)
3. [–í–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö](#–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ-–±–∞–∑—ã-–¥–∞–Ω–Ω—ã—Ö)
4. [–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞](#—ç–º–±–µ–¥–¥–∏–Ω–≥–∏-–∏-–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ-—Ç–µ–∫—Å—Ç–∞)
5. [–ü–æ–∏—Å–∫ –∏ —Ä–µ—Ç—Ä–∏–≤–∞–ª](#–ø–æ–∏—Å–∫-–∏-—Ä–µ—Ç—Ä–∏–≤–∞–ª)
6. [–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤](#–≥–µ–Ω–µ—Ä–∞—Ü–∏—è-–æ—Ç–≤–µ—Ç–æ–≤)
7. [–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã](#–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ-–ø—Ä–∏–º–µ—Ä—ã)
8. [–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞](#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è-–∏-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞)
9. [–õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏](#–ª—É—á—à–∏–µ-–ø—Ä–∞–∫—Ç–∏–∫–∏)

## –í–≤–µ–¥–µ–Ω–∏–µ –≤ RAG

**RAG (Retrieval-Augmented Generation)** ‚Äî —ç—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, —Å–æ—á–µ—Ç–∞—é—â–∞—è –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –≤–Ω–µ—à–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.

–≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç `LLM` –ø–æ–ª—É—á–∞—Ç—å –¥–æ—Å—Ç—É–ø –∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º –∑–Ω–∞–Ω–∏—è–º –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–º –¥–∞–Ω–Ω—ã–º, –Ω–µ –≤–∫–ª—é—á–µ–Ω–Ω—ã–º –≤ –∏—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ.

**–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ RAG:**

- –î–æ—Å—Ç—É–ø –∫ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã —Å —á–∞—Å—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- –°–Ω–∏–∂–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∞ "–≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π"
- –ì–∏–±–∫–æ—Å—Ç—å –∏ —Ä–∞—Å—à–∏—Ä—è–µ–º–æ—Å—Ç—å

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ RAG —Å–∏—Å—Ç–µ–º

### –ë–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ RAG:

```python
import torch
from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration

class RAGSystem:
    def __init__(self, model_name="facebook/rag-sequence-nq"):
        self.tokenizer = RagTokenizer.from_pretrained(model_name)
        self.retriever = RagRetriever.from_pretrained(
            model_name, 
            index_name="exact", 
            use_dummy_dataset=True
        )
        self.model = RagSequenceForGeneration.from_pretrained(
            model_name, 
            retriever=self.retriever
        )
    
    def generate_answer(self, question, context_docs=None):
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞
        input_dict = self.tokenizer.prepare_seq2seq_batch(
            question, 
            return_tensors="pt"
        )
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        with torch.no_grad():
            generated = self.model.generate(
                input_ids=input_dict["input_ids"],
                attention_mask=input_dict["attention_mask"],
                max_length=100
            )
        
        answer = self.tokenizer.batch_decode(
            generated, 
            skip_special_tokens=True
        )[0]
        
        return answer

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
rag_system = RAGSystem()
answer = rag_system.generate_answer("–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?")
```

### –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏:

```python
class CustomRAGSystem:
    def __init__(self, llm_model, embedding_model, vector_store):
        self.llm = llm_model
        self.embedding_model = embedding_model
        self.vector_store = vector_store
        self.prompt_template = """
        –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –≤ –∫–æ–Ω—Ü–µ.
        –ï—Å–ª–∏ –≤—ã –Ω–µ –∑–Ω–∞–µ—Ç–µ –æ—Ç–≤–µ—Ç–∞, –ø—Ä–æ—Å—Ç–æ —Å–∫–∞–∂–∏—Ç–µ, —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—Ç–µ, –Ω–µ –ø—ã—Ç–∞–π—Ç–µ—Å—å –ø—Ä–∏–¥—É–º–∞—Ç—å –æ—Ç–≤–µ—Ç.
        
        –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}
        
        –í–æ–ø—Ä–æ—Å: {question}
        
        –ü–æ–ª–µ–∑–Ω—ã–π –æ—Ç–≤–µ—Ç:
        """
    
    def retrieve_documents(self, query, k=5):
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        query_embedding = self.embedding_model.encode([query])
        results = self.vector_store.search(query_embedding[0], k=k)
        return [doc['text'] for doc in results]
    
    def generate_answer(self, question):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ—Ç—Ä–∏–≤–∞–ª–∞"""
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        docs = self.retrieve_documents(question)
        context = "\n\n".join(docs)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
        prompt = self.prompt_template.format(
            context=context,
            question=question
        )
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        response = self.llm.generate(prompt, max_tokens=200)
        return response.strip()

# –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã
class VectorStore:
    def __init__(self):
        self.documents = []
        self.embeddings = []
    
    def add_document(self, text, embedding):
        self.documents.append({"text": text, "embedding": embedding})
        self.embeddings.append(embedding)
    
    def search(self, query_embedding, k=5):
        """–ü–æ–∏—Å–∫ –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É"""
        similarities = []
        for i, doc_embedding in enumerate(self.embeddings):
            similarity = self.cosine_similarity(query_embedding, doc_embedding)
            similarities.append((similarity, i))
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities.sort(reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = []
        for similarity, idx in similarities[:k]:
            results.append({
                "text": self.documents[idx]["text"],
                "similarity": similarity
            })
        
        return results
    
    def cosine_similarity(self, a, b):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        dot_product = np.dot(a, b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        return dot_product / (norm_a * norm_b)
```

## –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

### FAISS (Facebook AI Similarity Search):

```python
import faiss
import numpy as np

class FaissVectorStore:
    def __init__(self, dimension, index_type="Flat"):
        self.dimension = dimension
        self.index = self._create_index(index_type)
        self.documents = []
        
    def _create_index(self, index_type):
        if index_type == "Flat":
            return faiss.IndexFlatL2(self.dimension)
        elif index_type == "IVF":
            quantizer = faiss.IndexFlatL2(self.dimension)
            return faiss.IndexIVFFlat(quantizer, self.dimension, 100)
        elif index_type == "HNSW":
            return faiss.IndexHNSWFlat(self.dimension, 32)
    
    def add_documents(self, texts, embeddings):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å"""
        self.documents.extend(texts)
        embeddings_array = np.array(embeddings).astype('float32')
        
        if isinstance(self.index, faiss.IndexIVFFlat):
            self.index.train(embeddings_array)
        
        self.index.add(embeddings_array)
    
    def search(self, query_embedding, k=5):
        """–ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π"""
        query_vector = np.array([query_embedding]).astype('float32')
        distances, indices = self.index.search(query_vector, k)
        
        results = []
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            if idx < len(self.documents):  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–Ω–∏—Ü
                results.append({
                    "text": self.documents[idx],
                    "distance": distance,
                    "similarity": 1 / (1 + distance)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –≤ —Å—Ö–æ–¥—Å—Ç–≤–æ
                })
        
        return results

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ FAISS
dimension = 768  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
vector_store = FaissVectorStore(dimension, index_type="IVF")

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
texts = ["–î–æ–∫—É–º–µ–Ω—Ç 1", "–î–æ–∫—É–º–µ–Ω—Ç 2", "–î–æ–∫—É–º–µ–Ω—Ç 3"]
embeddings = [[0.1, 0.2, ...], [0.3, 0.4, ...], [0.5, 0.6, ...]]  # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
vector_store.add_documents(texts, embeddings)
```

### ChromaDB:

```python
import chromadb
from chromadb.config import Settings

class ChromaVectorStore:
    def __init__(self, persist_directory="./chroma_db"):
        self.client = chromadb.Client(Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=persist_directory
        ))
        self.collection = self.client.create_collection("documents")
    
    def add_documents(self, texts, metadatas=None, ids=None):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if ids is None:
            ids = [f"doc_{i}" for i in range(len(texts))]
        
        self.collection.add(
            documents=texts,
            metadatas=metadatas or [{} for _ in texts],
            ids=ids
        )
    
    def search(self, query_text, n_results=5):
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        results = self.collection.query(
            query_texts=[query_text],
            n_results=n_results
        )
        
        return [
            {
                "text": doc,
                "metadata": meta,
                "distance": dist
            }
            for doc, meta, dist in zip(
                results['documents'][0],
                results['metadatas'][0],
                results['distances'][0]
            )
        ]
    
    def delete_documents(self, ids):
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        self.collection.delete(ids=ids)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ChromaDB
chroma_store = ChromaVectorStore("./my_chroma_db")
chroma_store.add_documents(
    texts=["–ü–µ—Ä–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç", "–í—Ç–æ—Ä–æ–π –¥–æ–∫—É–º–µ–Ω—Ç"],
    metadatas=[{"source": "book1"}, {"source": "book2"}]
)
```

### Pinecone:

```python
import pinecone

class PineconeVectorStore:
    def __init__(self, api_key, environment, index_name):
        pinecone.init(api_key=api_key, environment=environment)
        self.index_name = index_name
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –µ—Å–ª–∏ –æ–Ω –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if index_name not in pinecone.list_indexes():
            pinecone.create_index(
                name=index_name,
                dimension=768,
                metric="cosine"
            )
        
        self.index = pinecone.Index(index_name)
    
    def add_documents(self, vectors, metadata=None, ids=None):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if ids is None:
            ids = [f"vec_{i}" for i in range(len(vectors))]
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Pinecone
        vectors_to_upsert = [
            (id_, vector, meta) 
            for id_, vector, meta in zip(ids, vectors, metadata or [{}] * len(vectors))
        ]
        
        self.index.upsert(vectors=vectors_to_upsert)
    
    def search(self, query_vector, top_k=5, filter_metadata=None):
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤"""
        query_response = self.index.query(
            vector=query_vector,
            top_k=top_k,
            include_metadata=True,
            filter=filter_metadata
        )
        
        results = []
        for match in query_response['matches']:
            results.append({
                "id": match['id'],
                "text": match['metadata'].get('text', ''),
                "score": match['score'],
                "metadata": match['metadata']
            })
        
        return results

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Pinecone
pinecone_store = PineconeVectorStore(
    api_key="your-api-key",
    environment="us-west1-gcp",
    index_name="my-rag-index"
)
```

## –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞

### Sentence Transformers:

```python
from sentence_transformers import SentenceTransformer

class EmbeddingModel:
    def __init__(self, model_name="all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
    
    def encode(self, texts):
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä—ã"""
        if isinstance(texts, str):
            texts = [texts]
        
        embeddings = self.model.encode(
            texts,
            convert_to_tensor=True,
            normalize_embeddings=True
        )
        
        return embeddings.cpu().numpy()
    
    def encode_batch(self, texts, batch_size=32):
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_embeddings = self.encode(batch)
            embeddings.extend(batch_embeddings)
        return embeddings

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
embedding_model = EmbeddingModel("all-MiniLM-L6-v2")
vectors = embedding_model.encode(["–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è"])
```

### OpenAI Embeddings:

```python
import openai

class OpenAIEmbeddingModel:
    def __init__(self, api_key, model="text-embedding-ada-002"):
        openai.api_key = api_key
        self.model = model
    
    def encode(self, texts):
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é OpenAI"""
        if isinstance(texts, str):
            texts = [texts]
        
        response = openai.Embedding.create(
            input=texts,
            model=self.model
        )
        
        embeddings = [item['embedding'] for item in response['data']]
        return embeddings

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
openai_model = OpenAIEmbeddingModel("your-api-key")
embeddings = openai_model.encode(["–¢–µ–∫—Å—Ç –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è"])
```

### –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞:

```python
import re
from typing import List

class TextProcessor:
    def __init__(self):
        self.chunk_size = 500
        self.chunk_overlap = 100
    
    def clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text)
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        text = re.sub(r'[^\w\s.,!?;:()-]', '', text)
        return text.strip()
    
    def split_text(self, text: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        sentences = re.split(r'[.!?]+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–∞
            if len(current_chunk) + len(sentence) <= self.chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–∞–Ω–∫–∞
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def process_document(self, text: str) -> List[str]:
        """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        cleaned_text = self.clean_text(text)
        chunks = self.split_text(cleaned_text)
        return chunks

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
processor = TextProcessor()
chunks = processor.process_document(large_document_text)
```

## –ü–æ–∏—Å–∫ –∏ —Ä–µ—Ç—Ä–∏–≤–∞–ª

### –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —Ä–µ—Ç—Ä–∏–≤–∞–ª:

```python
class MultiStageRetriever:
    def __init__(self, vector_store, embedding_model, keyword_retriever=None):
        self.vector_store = vector_store
        self.embedding_model = embedding_model
        self.keyword_retriever = keyword_retriever
    
    def hybrid_search(self, query, k=10, alpha=0.7):
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏ –∫–ª—é—á–µ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
        alpha: –≤–µ—Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (0-1)
        """
        # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        query_embedding = self.embedding_model.encode([query])[0]
        vector_results = self.vector_store.search(query_embedding, k=k*2)
        
        # –ö–ª—é—á–µ–≤–æ–π –ø–æ–∏—Å–∫ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        if self.keyword_retriever:
            keyword_results = self.keyword_retriever.search(query, k=k*2)
        else:
            keyword_results = []
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        combined_results = self._combine_results(
            vector_results, 
            keyword_results, 
            alpha
        )
        
        return combined_results[:k]
    
    def _combine_results(self, vector_results, keyword_results, alpha):
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –≤–µ—Å–∞–º–∏"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–æ–∫
        vector_scores = [(result['text'], result['similarity']) 
                        for result in vector_results]
        keyword_scores = [(result['text'], result['score']) 
                         for result in keyword_results]
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        vector_dict = dict(vector_scores)
        keyword_dict = dict(keyword_scores)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        all_texts = set(vector_dict.keys()) | set(keyword_dict.keys())
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
        combined_scores = []
        for text in all_texts:
            vector_score = vector_dict.get(text, 0)
            keyword_score = keyword_dict.get(text, 0)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            combined_score = alpha * vector_score + (1 - alpha) * keyword_score
            combined_scores.append((text, combined_score))
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é
        combined_scores.sort(key=lambda x: x[1], reverse=True)
        
        return [{'text': text, 'score': score} for text, score in combined_scores]

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
retriever = MultiStageRetriever(vector_store, embedding_model)
results = retriever.hybrid_search("–í–æ–ø—Ä–æ—Å –æ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏", k=5)
```

### –†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é LLM:

```python
class LLMReranker:
    def __init__(self, llm_model):
        self.llm = llm_model
    
    def rerank(self, query, documents, k=5):
        """–ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM"""
        reranked_docs = []
        
        for doc in documents:
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            prompt = f"""
            –û—Ü–µ–Ω–∏—Ç–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å.
            –û—Ü–µ–Ω–∏—Ç–µ –ø–æ —à–∫–∞–ª–µ –æ—Ç 1 –¥–æ 10, –≥–¥–µ 10 - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ.
            
            –í–æ–ø—Ä–æ—Å: {query}
            
            –î–æ–∫—É–º–µ–Ω—Ç: {doc['text']}
            
            –û—Ç–≤–µ—Ç—å—Ç–µ —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–º –æ—Ç 1 –¥–æ 10:
            """
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –æ—Ç LLM
            response = self.llm.generate(prompt, max_tokens=10)
            try:
                relevance_score = float(response.strip())
            except ValueError:
                relevance_score = 0.0
            
            doc['rerank_score'] = relevance_score
            reranked_docs.append(doc)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –Ω–æ–≤–æ–π –æ—Ü–µ–Ω–∫–µ
        reranked_docs.sort(key=lambda x: x['rerank_score'], reverse=True)
        
        return reranked_docs[:k]

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
reranker = LLMReranker(llm_model)
final_results = reranker.rerank(query, retrieved_docs, k=5)
```

## –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤

### Prompt Engineering –¥–ª—è RAG:

```python
class RAGPromptBuilder:
    def __init__(self):
        self.templates = {
            'qa': """
            –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å.
            –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Å–∫–∞–∂–∏—Ç–µ "–Ø –Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö".
            
            –ö–æ–Ω—Ç–µ–∫—Å—Ç:
            {context}
            
            –í–æ–ø—Ä–æ—Å: {question}
            
            –û—Ç–≤–µ—Ç:
            """,
            
            'summarization': """
            –°–æ–∑–¥–∞–π—Ç–µ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Å–ª–µ–¥—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
            
            –î–æ–∫—É–º–µ–Ω—Ç—ã:
            {context}
            
            –†–µ–∑—é–º–µ:
            """,
            
            'comparison': """
            –°—Ä–∞–≤–Ω–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
            
            –î–æ–∫—É–º–µ–Ω—Ç—ã:
            {context}
            
            –°—Ä–∞–≤–Ω–µ–Ω–∏–µ:
            """
        }
    
    def build_prompt(self, template_name, context, question=None):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –ø–æ —à–∞–±–ª–æ–Ω—É"""
        if template_name not in self.templates:
            raise ValueError(f"Unknown template: {template_name}")
        
        template = self.templates[template_name]
        
        if template_name == 'qa':
            return template.format(context=context, question=question)
        else:
            return template.format(context=context)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
prompt_builder = RAGPromptBuilder()
qa_prompt = prompt_builder.build_prompt(
    'qa', 
    context='\n\n'.join([doc['text'] for doc in relevant_docs]),
    question=user_question
)
```

### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:

```python
class CitationAwareGenerator:
    def __init__(self, llm_model):
        self.llm = llm_model
    
    def generate_with_citations(self, question, documents):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –Ω–æ–º–µ—Ä–∞–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        context_parts = []
        for i, doc in enumerate(documents, 1):
            context_parts.append(f"[{i}] {doc['text']}")
        
        context = '\n\n'.join(context_parts)
        
        prompt = f"""
        –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å. 
        –¶–∏—Ç–∏—Ä—É–π—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö [1], [2], –∏ —Ç.–¥.
        –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –±–µ—Ä–µ—Ç—Å—è –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, —É–∫–∞–∂–∏—Ç–µ –≤—Å–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –Ω–æ–º–µ—Ä–∞.
        
        –î–æ–∫—É–º–µ–Ω—Ç—ã:
        {context}
        
        –í–æ–ø—Ä–æ—Å: {question}
        
        –û—Ç–≤–µ—Ç —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º:
        """
        
        response = self.llm.generate(prompt, max_tokens=300)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        citations = self._extract_citations(response, documents)
        
        return {
            'answer': response,
            'citations': citations
        }
    
    def _extract_citations(self, response, documents):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π"""
        import re
        
        # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö
        citation_pattern = r'\[(\d+(?:,\s*\d+)*)\]'
        matches = re.findall(citation_pattern, response)
        
        citations = []
        for match in matches:
            # –†–∞–∑–±–æ—Ä –Ω–æ–º–µ—Ä–æ–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            source_numbers = [int(x.strip()) for x in match.split(',')]
            
            for num in source_numbers:
                if 1 <= num <= len(documents):
                    citations.append({
                        'source_number': num,
                        'text': documents[num-1]['text'][:200] + "...",
                        'metadata': documents[num-1].get('metadata', {})
                    })
        
        return citations

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
generator = CitationAwareGenerator(llm_model)
result = generator.generate_with_citations(question, retrieved_docs)
print(f"–û—Ç–≤–µ—Ç: {result['answer']}")
print("–¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
for citation in result['citations']:
    print(f"[{citation['source_number']}] {citation['text']}")
```

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã

### –ü—Ä–∏–º–µ—Ä ‚Ññ1: –ß–∞—Ç-–±–æ—Ç —Å RAG –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏

```python
class DocumentationChatbot:
    def __init__(self, docs_path, llm_model, embedding_model):
        self.vector_store = self._build_vector_store(docs_path, embedding_model)
        self.llm = llm_model
        self.embedding_model = embedding_model
    
    def _build_vector_store(self, docs_path, embedding_model):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        vector_store = FaissVectorStore(dimension=embedding_model.dimension)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        for filename in os.listdir(docs_path):
            if filename.endswith('.txt'):
                with open(os.path.join(docs_path, filename), 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
                chunks = self._split_text(content)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                embeddings = embedding_model.encode(chunks)
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
                vector_store.add_documents(chunks, embeddings)
        
        return vector_store
    
    def _split_text(self, text, chunk_size=500):
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        sentences = text.split('.')
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) < chunk_size:
                current_chunk += sentence + "."
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "."
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def chat(self, question):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        query_embedding = self.embedding_model.encode([question])[0]
        docs = self.vector_store.search(query_embedding, k=3)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context = '\n\n'.join([doc['text'] for doc in docs])
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        prompt = f"""
        –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–ª–µ–¥—É—é—â—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å:
        
        –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:
        {context}
        
        –í–æ–ø—Ä–æ—Å: {question}
        
        –û—Ç–≤–µ—Ç:
        """
        
        response = self.llm.generate(prompt, max_tokens=200)
        return response.strip()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
chatbot = DocumentationChatbot('./docs', llm_model, embedding_model)
answer = chatbot.chat("–ö–∞–∫ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —ç—Ç–æ—Ç –ø–∞–∫–µ—Ç?")
```

### –ü—Ä–∏–º–µ—Ä ‚Ññ2: –°–∏—Å—Ç–µ–º–∞ –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π

```python
class ResearchQA:
    def __init__(self, papers_directory):
        self.vector_store = ChromaVectorStore("./research_papers")
        self.embedding_model = EmbeddingModel("all-MiniLM-L6-v2")
        self.llm = self._load_llm()
        self._index_papers(papers_directory)
    
    def _load_llm(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        model = AutoModelForCausalLM.from_pretrained("gpt2")
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        return {"model": model, "tokenizer": tokenizer}
    
    def _index_papers(self, papers_dir):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"""
        import json
        
        for filename in os.listdir(papers_dir):
            if filename.endswith('.json'):
                with open(os.path.join(papers_dir, filename), 'r') as f:
                    paper = json.load(f)
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π —Å—Ç–∞—Ç—å–∏
                sections = [
                    f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {paper.get('title', '')}",
                    f"–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è: {paper.get('abstract', '')}",
                    f"–í–≤–µ–¥–µ–Ω–∏–µ: {paper.get('introduction', '')}",
                    f"–ú–µ—Ç–æ–¥—ã: {paper.get('methods', '')}",
                    f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {paper.get('results', '')}",
                    f"–ó–∞–∫–ª—é—á–µ–Ω–∏–µ: {paper.get('conclusion', '')}"
                ]
                
                # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Å–µ–∫—Ü–∏–∏
                for i, section in enumerate(sections):
                    if section.split(': ', 1)[1].strip():  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ—Ç—É
                        embedding = self.embedding_model.encode([section])[0]
                        self.vector_store.add_documents(
                            [section],
                            [{"paper_id": paper.get('id'), "section": i}],
                            [f"{paper.get('id')}_{i}"]
                        )
    
    def answer_question(self, question, paper_filter=None):
        """–û—Ç–≤–µ—Ç –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å"""
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å–µ–∫—Ü–∏–π
        query_embedding = self.embedding_model.encode([question])[0]
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å—Ç–∞—Ç—å—è–º –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
        filter_dict = {"paper_id": paper_filter} if paper_filter else None
        relevant_sections = self.vector_store.search(
            question, 
            n_results=10,
            filter_metadata=filter_dict
        )
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context = '\n\n'.join([sec['text'] for sec in relevant_sections])
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        prompt = f"""
        –ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö –Ω–∞—É—á–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –æ—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å.
        –¶–∏—Ç–∏—Ä—É–π—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [–Ω–æ–º–µ—Ä_—Å—Ç–∞—Ç—å–∏].
        
        –ú–∞—Ç–µ—Ä–∏–∞–ª—ã:
        {context}
        
        –í–æ–ø—Ä–æ—Å: {question}
        
        –û—Ç–≤–µ—Ç:
        """
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é LLM
        response = self._generate_with_llm(prompt)
        return self._format_response(response, relevant_sections)
    
    def _generate_with_llm(self, prompt):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM"""
        tokenizer = self.llm["tokenizer"]
        model = self.llm["model"]
        
        inputs = tokenizer.encode(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=inputs.shape[1] + 200,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
        
        return tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):]
    
    def _format_response(self, response, sections):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        return {
            "answer": response,
            "sources": [
                {
                    "paper_id": sec['metadata']['paper_id'],
                    "section": sec['metadata']['section'],
                    "text": sec['text'][:100] + "..."
                } for sec in sections
            ]
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
research_qa = ResearchQA("./papers")
result = research_qa.answer_question(
    "–ö–∞–∫–æ–≤—ã –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤?",
    paper_filter="transformer_survey_2023"
)
```

## –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞:

```python
class OptimizedRetriever:
    def __init__(self, vector_store, embedding_model):
        self.vector_store = vector_store
        self.embedding_model = embedding_model
        self.cache = {}  # –ö—ç—à –¥–ª—è —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    
    def optimized_search(self, query, k=5, use_cache=True):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        if use_cache and query in self.cache:
            return self.cache[query]
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
        processed_query = self._preprocess_query(query)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        query_embedding = self.embedding_model.encode([processed_query])[0]
        
        # –ü–æ–∏—Å–∫ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        results = self.vector_store.search(
            query_embedding, 
            k=k,
            search_params={"ef_search": 100}  # –î–ª—è HNSW –∏–Ω–¥–µ–∫—Å–æ–≤
        )
        
        # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        filtered_results = self._filter_results(results)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
        if use_cache:
            self.cache[query] = filtered_results
        
        return filtered_results
    
    def _preprocess_query(self, query):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Ç.–¥.
        import re
        query = re.sub(r'[^\w\s]', '', query.lower())
        return ' '.join(query.split())  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
    
    def _filter_results(self, results):
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É"""
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        seen_texts = set()
        filtered = []
        
        for result in results:
            text = result['text'].strip()
            if text and text not in seen_texts:
                seen_texts.add(text)
                filtered.append(result)
        
        return filtered

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
optimized_retriever = OptimizedRetriever(vector_store, embedding_model)
results = optimized_retriever.optimized_search("—Å–ª–æ–∂–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å")
```

### –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:

```python
class AdaptiveRAG:
    def __init__(self, base_retriever, llm):
        self.base_retriever = base_retriever
        self.llm = llm
        self.performance_history = []
    
    def adaptive_search(self, query, target_recall=0.8):
        """–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        k_values = [3, 5, 10, 15, 20]
        best_k = 5
        best_score = 0
        
        # –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ k
        for k in k_values:
            results = self.base_retriever.search(query, k=k)
            score = self._evaluate_results(query, results)
            
            if score > best_score:
                best_score = score
                best_k = k
            
            # –†–∞–Ω–Ω–∏–π –≤—ã—Ö–æ–¥ –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ü–µ–ª—å
            if score >= target_recall:
                break
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ k
        final_results = self.base_retriever.search(query, k=best_k)
        return final_results, best_k
    
    def _evaluate_results(self, query, results):
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LLM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        context = '\n\n'.join([r['text'] for r in results[:3]])
        
        prompt = f"""
        –û—Ü–µ–Ω–∏—Ç–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å–ª–µ–¥—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å.
        –û—Ü–µ–Ω–∏—Ç–µ –ø–æ —à–∫–∞–ª–µ –æ—Ç 0 –¥–æ 1, –≥–¥–µ 1 - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ.
        
        –í–æ–ø—Ä–æ—Å: {query}
        
        –î–æ–∫—É–º–µ–Ω—Ç—ã:
        {context}
        
        –û—Ç–≤–µ—Ç—å—Ç–µ —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–º –æ—Ç 0 –¥–æ 1:
        """
        
        response = self.llm.generate(prompt, max_tokens=10)
        try:
            return float(response.strip())
        except ValueError:
            return 0.0
    
    def update_performance_history(self, query, actual_k, user_feedback):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self.performance_history.append({
            'query': query,
            'k_used': actual_k,
            'feedback': user_feedback,
            'timestamp': datetime.now()
        })

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
adaptive_rag = AdaptiveRAG(base_retriever, llm)
results, optimal_k = adaptive_rag.adaptive_search("—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å")
```

## –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏

### 1. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö:

```python
class DataQualityManager:
    def __init__(self):
        self.quality_threshold = 0.7
    
    def assess_document_quality(self, text):
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        metrics = {
            'length_score': self._length_score(text),
            'coherence_score': self._coherence_score(text),
            'diversity_score': self._diversity_score(text)
        }
        
        overall_score = sum(metrics.values()) / len(metrics)
        return overall_score, metrics
    
    def _length_score(self, text):
        """–û—Ü–µ–Ω–∫–∞ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞"""
        word_count = len(text.split())
        if word_count < 50:
            return 0.3
        elif word_count < 200:
            return 0.7
        elif word_count < 1000:
            return 1.0
        else:
            return 0.8  # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –º–æ–≥—É—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ –ø–æ–ª–µ–∑–Ω—ã
    
    def _coherence_score(self, text):
        """–û—Ü–µ–Ω–∫–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
        words = text.lower().split()
        if len(words) == 0:
            return 0
        
        unique_ratio = len(set(words)) / len(words)
        return 1 - abs(unique_ratio - 0.5)  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –æ–∫–æ–ª–æ 0.5
    
    def _diversity_score(self, text):
        """–û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è"""
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —á–∞—Å—Ç–µ–π —Ä–µ—á–∏
        import nltk
        try:
            tokens = nltk.word_tokenize(text)
            pos_tags = nltk.pos_tag(tokens)
            tag_types = set([tag for word, tag in pos_tags])
            return min(len(tag_types) / 10, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        except:
            return 0.5  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
quality_manager = DataQualityManager()
score, metrics = quality_manager.assess_document_quality(document_text)
if score >= quality_manager.quality_threshold:
    # –î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ –∏–Ω–¥–µ–∫—Å
    pass
```

### 2. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:

```python
class RAGMonitor:
    def __init__(self):
        self.metrics = {
            'query_count': 0,
            'avg_response_time': 0,
            'retrieval_accuracy': 0,
            'generation_quality': 0
        }
        self.query_log = []
    
    def log_query(self, query, response_time, retrieval_results, final_answer):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∏ –º–µ—Ç—Ä–∏–∫"""
        self.metrics['query_count'] += 1
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞
        old_avg = self.metrics['avg_response_time']
        new_avg = (old_avg * (self.metrics['query_count'] - 1) + response_time) / self.metrics['query_count']
        self.metrics['avg_response_time'] = new_avg
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π
        self.query_log.append({
            'query': query,
            'response_time': response_time,
            'retrieved_docs_count': len(retrieval_results),
            'answer_length': len(final_answer),
            'timestamp': datetime.now()
        })
    
    def calculate_metrics(self, ground_truth_data):
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"""
        if not ground_truth_data:
            return
        
        retrieval_correct = 0
        total_queries = len(ground_truth_data)
        
        for query_data in ground_truth_data:
            query = query_data['query']
            expected_docs = set(query_data['relevant_docs'])
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            actual_results = self._get_actual_results(query)
            actual_docs = set([r['id'] for r in actual_results])
            
            # –†–∞—Å—á–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–µ—Ç—Ä–∏–≤–∞–ª–∞
            if expected_docs & actual_docs:  # –ï—Å—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
                retrieval_correct += 1
        
        self.metrics['retrieval_accuracy'] = retrieval_correct / total_queries
    
    def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        report = f"""
        RAG System Performance Report
        ============================
        
        Total Queries: {self.metrics['query_count']}
        Average Response Time: {self.metrics['avg_response_time']:.2f}s
        Retrieval Accuracy: {self.metrics['retrieval_accuracy']:.2%}
        Generation Quality Score: {self.metrics['generation_quality']:.2f}/1.0
        
        Recent Performance Trends:
        """
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        recent_queries = self.query_log[-10:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∑–∞–ø—Ä–æ—Å–æ–≤
        avg_recent_time = sum(q['response_time'] for q in recent_queries) / len(recent_queries)
        
        report += f"Average time for recent queries: {avg_recent_time:.2f}s\n"
        
        return report

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
monitor = RAGMonitor()
# –í–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
monitor.log_query(query, response_time, retrieved_docs, final_answer)
# –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏
performance_report = monitor.generate_report()
```

### 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å:

```python
class RobustRAG:
    def __init__(self, retriever, generator, fallback_responses=None):
        self.retriever = retriever
        self.generator = generator
        self.fallback_responses = fallback_responses or self._default_fallbacks()
        self.error_count = 0
        self.max_errors = 5
    
    def _default_fallbacks(self):
        return {
            'no_results': "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.",
            'generation_failed': "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–æ–∑–Ω–∏–∫–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.",
            'retrieval_failed': "–ò–∑–≤–∏–Ω–∏—Ç–µ, —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."
        }
    
    def robust_query(self, question):
        """–û—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        try:
            # –≠—Ç–∞–ø 1: –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            try:
                docs = self.retriever.search(question, k=5)
                if not docs:
                    return self.fallback_responses['no_results']
            except Exception as e:
                self._handle_error("retrieval", e)
                return self.fallback_responses['retrieval_failed']
            
            # –≠—Ç–∞–ø 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            try:
                answer = self.generator.generate(question, docs)
                if not answer or len(answer.strip()) < 5:
                    return self.fallback_responses['generation_failed']
            except Exception as e:
                self._handle_error("generation", e)
                return self.fallback_responses['generation_failed']
            
            return answer
            
        except Exception as e:
            self._handle_error("general", e)
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    
    def _handle_error(self, error_type, exception):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫"""
        self.error_count += 1
        print(f"Error in {error_type}: {str(exception)}")
        
        # –†–µ–∞–∫—Ü–∏—è –Ω–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ –ª–∏–º–∏—Ç–∞ –æ—à–∏–±–æ–∫
        if self.error_count >= self.max_errors:
            self._trigger_failover()
    
    def _trigger_failover(self):
        """–ê–∫—Ç–∏–≤–∞—Ü–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        print("Triggering failover mechanism...")
        # –ó–¥–µ—Å—å –º–æ–∂–µ—Ç –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É
        self.error_count = 0  # –°–±—Ä–æ—Å —Å—á–µ—Ç—á–∏–∫–∞ –ø–æ—Å–ª–µ failover

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
robust_rag = RobustRAG(retriever, generator)
answer = robust_rag.robust_query("—Å–ª–æ–∂–Ω—ã–π –≤–æ–ø—Ä–æ—Å")
```

---

#### üíº –ê–≤—Ç–æ—Ä: –î—É–ø–ª–µ–π –ú–∞–∫—Å–∏–º –ò–≥–æ—Ä–µ–≤–∏—á

### üì≤ –ö–æ–Ω—Ç–∞–∫—Ç—ã:

- **Telegram ‚Ññ1:** [@quadd4rv1n7](https://t.me/quadd4rv1n7)
- **Telegram ‚Ññ2:** [@dupley_maxim_1999](https://t.me/dupley_maxim_1999)

üìÖ **–î–∞—Ç–∞:** 26.01.2026

‚ñ∂Ô∏è –í–µ—Ä—Å–∏—è 1.0

---
> üìß **–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤—É:** maksimqwe42@mail.ru

---

### üíº –ü—Ä–æ—Ñ–∏–ª—å –Ω–∞ Profi.ru
[![Profi.ru Profile](https://img.shields.io/badge/Profi.ru-–î—É–ø–ª–µ–π%20–ú.–ò.-FF6B35?style=for-the-badge)](https://profi.ru/profile/DupleyMI)

> –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –∏ —É—Å–ª—É–≥–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ Profi.ru

---

### üìö –£—Å–ª—É–≥–∏ –æ–±—É—á–µ–Ω–∏—è
[![–û–±—É—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º –∏ —è–∑—ã–∫–∞–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ Kwork](https://img.shields.io/badge/Kwork-–û–±—É—á–µ–Ω–∏–µ%20–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é-blue?style=for-the-badge&logo=kwork)](https://kwork.ru/usability-testing/42465951/–æ–±—É—á–µ–Ω–∏–µ-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º-–∏-—è–∑—ã–∫–∞–º-–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è)

> –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º –∏ —è–∑—ã–∫–∞–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –∏ –∫—É—Ä—Å—ã –æ—Ç –æ–ø—ã—Ç–Ω–æ–≥–æ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è.

---

### üè´ –û —à–∫–æ–ª–µ
[![Website](https://img.shields.io/badge/Maestro7IT-school--maestro7it.ru-darkgreen?style=for-the-badge)](https://school-maestro7it.ru/)

> –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è —à–∫–æ–ª–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∞—è—Å—è –Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.